[
  {
    "question": "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
    "options": [
      "AWS Elastic Load Balancing (ELB)",
      "Amazon Route 53",
      "Amazon CloudFront",
      "AWS Global Accelerator"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A telecom company operates thousands of hardware devices like switches, routers, cables, etc. The real-time status data for these devices must be fed into a communications application for notifications. Simultaneously, another analytics application needs to read the same real-time status data and analyze all the connecting lines that may go down because of any device failures. As an AWS Certified Solutions Architect – Associate, which of the following solutions would you suggest, so that both the applications can consume the real-time status data concurrently?",
    "options": [
      "Amazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES)",
      "Amazon Kinesis Data Streams",
      "Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)",
      "Amazon Simple Notification Service (SNS)"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company is in the process of migrating its on-premises SMB file shares to AWS so the company can get out of the business of managing multiple file servers across dozens of offices. The company has 200 terabytes of data in its file servers. The existing on-premises applications and native Windows workloads should continue to have low latency access to this data which needs to be stored on a file system service without any disruptions after the migration. The company also wants any new applications deployed on AWS to have access to this migrated data. Which of the following is the best solution to meet this requirement?",
    "options": [
      "Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
    "options": [
      "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket",
      "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files",
      "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets",
      "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
    "options": [
      "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days",
      "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days",
      "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days",
      "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A social photo-sharing company uses Amazon Simple Storage Service (Amazon S3) to store the images uploaded by the users. These images are kept encrypted in Amazon S3 by using AWS Key Management Service (AWS KMS) and the company manages its own AWS KMS keys for encryption. A member of the DevOps team accidentally deleted the AWS KMS key a day ago, thereby rendering the user's photo data unrecoverable. You have been contacted by the company to consult them on possible solutions to this crisis. As a solutions architect, which of the following steps would you recommend to solve this issue?",
    "options": [
      "As the AWS KMS key was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the KMS key deletion and recover the key",
      "The company should issue a notification on its web application informing the users about the loss of their data",
      "The AWS KMS key can be recovered by the AWS root account user",
      "Contact AWS support to retrieve the AWS KMS key from their backup"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations. Which of the following services can be used to support this requirement?",
    "options": [
      "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
      "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
      "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda",
      "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two) Correct selection Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights Your selection is correct Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights Overall explanation Correct options: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution You can use georestriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution. When a user requests your content, Amazon CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following: Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries. Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. So this option is also correct. Amazon Route 53 Routing Policy Overview: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html Incorrect options: Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights - Use latency-based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server. Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records. Weighted routing or failover routing or latency routing cannot be used to restrict the distribution of content to only the locations in which you have distribution rights. So all three options above are incorrect. References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
    "options": [
      "Upload the compressed file in a single operation",
      "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket",
      "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)",
      "Upload the compressed file using multipart upload"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Which of the following feature of an Amazon S3 bucket can only be suspended and not disabled once it have been enabled?",
    "options": [
      "Versioning",
      "Server Access Logging",
      "Static Website Hosting",
      "Requester Pays"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
    "options": [
      "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class",
      "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class",
      "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class",
      "Store the intermediary query results in Amazon S3 Standard storage class"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A gaming company uses Amazon Aurora as its primary database service. The company has now deployed 5 multi-AZ read replicas to increase the read throughput and for use as failover target. The replicas have been assigned the following failover priority tiers and corresponding instance sizes are given in parentheses: tier-1 (16 terabytes), tier-1 (32 terabytes), tier-10 (16 terabytes), tier-15 (16 terabytes), tier-15 (32 terabytes). In the event of a failover, Amazon Aurora will promote which of the following read replicas?",
    "options": [
      "Tier-1 (32 terabytes)",
      "Tier-15 (32 terabytes)",
      "Tier-1 (16 terabytes)",
      "Tier-10 (16 terabytes)"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A geological research agency maintains the seismological data for the last 100 years. The data has a velocity of 1GB per minute. You would like to store the data with only the most relevant attributes to build a predictive model for earthquakes. What AWS services would you use to build the most cost-effective solution with the LEAST amount of infrastructure maintenance?",
    "options": [
      "Ingest the data in Amazon Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to Amazon S3",
      "Ingest the data in Amazon Kinesis Data Streams and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3",
      "Ingest the data in a Spark Streaming Cluster on Amazon EMR and use Spark Streaming transformations before writing to Amazon S3",
      "Ingest the data in Amazon Kinesis Data Firehose and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
    "options": [
      "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3",
      "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3",
      "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3",
      "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
    "options": [
      "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS",
      "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS",
      "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS",
      "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
    "options": [
      "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3",
      "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3",
      "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3",
      "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two) ?",
    "options": [
      "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering",
      "Amazon S3 Standard => Amazon S3 Intelligent-Tiering",
      "Your selection is correct",
      "Amazon S3 Intelligent-Tiering => Amazon S3 Standard"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two) Your selection is correct Cold Hard disk drive (sc1) Your selection is correct Throughput Optimized Hard disk drive (st1) Provisioned IOPS Solid state drive (io1) Instance Store General Purpose Solid State Drive (gp2) Overall explanation Correct options: Throughput Optimized Hard disk drive (st1) Cold Hard disk drive (sc1) The Amazon EBS volume types fall into two categories: Solid state drive (SSD) backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS. Hard disk drive (HDD) backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS. Throughput Optimized HDD (st1) and Cold HDD (sc1) volume types CANNOT be used as a boot volume, so these two options are correct. Please see this detailed overview of the volume types for Amazon EBS volumes. via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html Incorrect options: General Purpose Solid State Drive (gp2) Provisioned IOPS Solid state drive (io1) Instance Store General Purpose SSD (gp2), Provisioned IOPS SSD (io1), and Instance Store can be used as a boot volume. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
    "options": [
      "Leverage Amazon Athena with Amazon S3",
      "Leverage Amazon API Gateway with AWS Lambda",
      "Leverage Amazon QuickSight with Amazon Redshift",
      "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
    "options": [
      "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B",
      "1 Amazon EC2 instance and 1 AMI exist in Region B",
      "1 Amazon EC2 instance and 2 AMIs exist in Region B",
      "1 Amazon EC2 instance and 1 snapshot exist in Region B"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two) Correct selection Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements Your selection is correct Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements Overall explanation Correct options: Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis can be used to power the live leaderboard, so this option is correct. Amazon ElastiCache for Redis Overview: Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard. Incorrect options: Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune is not an in-memory database, so this option is not correct. Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements - DynamoDB is not an in-memory database, so this option is not correct. Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database, so this option is not correct. References: https://aws.amazon.com/elasticache/ https://aws.amazon.com/elasticache/redis/ https://aws.amazon.com/dynamodb/dax/ Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
    "options": [
      "AWS Glue",
      "Amazon FSx for Lustre",
      "Amazon FSx for Windows File Server",
      "Amazon EMR"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
    "options": [
      "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively",
      "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput",
      "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures",
      "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Amazon CloudFront offers a multi-tier cache in the form of regional edge caches that improve latency. However, there are certain content types that bypass the regional edge cache, and go directly to the origin. Which of the following content types skip the regional edge cache? (Select two) Static content such as style sheets, JavaScript files Your selection is correct Dynamic content, as determined at request time (cache-behavior configured to forward all headers) User-generated videos E-commerce assets such as product photos Your selection is correct Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin Overall explanation Correct options: Dynamic content, as determined at request time (cache-behavior configured to forward all headers) Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Dynamic content, as determined at request time (cache-behavior configured to forward all headers), does not flow through regional edge caches, but goes directly to the origin. So this option is correct. Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin from the POPs and do not proxy through the regional edge caches. So this option is also correct. How Amazon CloudFront Delivers Content: via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html Incorrect Options: E-commerce assets such as product photos User-generated videos Static content such as style sheets, JavaScript files The following type of content flows through the regional edge caches - user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos and static content such as style sheets, JavaScript files. Hence these three options are not correct. Reference: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A video analytics organization has been acquired by a leading media company. The analytics organization has 10 independent applications with an on-premises data footprint of about 70 Terabytes for each application. The CTO of the media company has set a timeline of two weeks to carry out the data migration from on-premises data center to AWS Cloud and establish connectivity. Which of the following are the MOST cost-effective options for completing the data transfer and establishing connectivity? (Select two) Your selection is incorrect Order 1 AWS Snowmobile to complete the one-time data transfer Correct selection Setup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud Setup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud Order 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer Correct selection Order 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer Overall explanation Correct options: Order 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 Terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gigabytes network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80 Terabytes of data, you can order 10 such devices to take care of the data transfer for all applications. Exam Alert: The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80 Terabytes of storage space. Setup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. Therefore this option is the right fit for the given use-case as the connectivity can be easily established within the given timeframe. Incorrect options: Order 1 AWS Snowmobile to complete the one-time data transfer - Each AWS Snowmobile has a total capacity of up to 100 petabytes. To migrate large datasets of 10 petabytes or more in a single location, you should use AWS Snowmobile. For datasets less than 10 petabytes or distributed in multiple locations, you should use Snowball. So AWS Snowmobile is not the right fit for this use-case. Setup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Direct Connect involves significant monetary investment and takes at least a month to set up, therefore it's not the correct fit for this use-case. Order 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer - As the data-transfer can be completed with just 10 AWS Snowball Edge Storage Optimized devices, there is no need to order 70 devices. References: https://aws.amazon.com/snowball/faqs/ https://aws.amazon.com/vpn/ https://aws.amazon.com/snowmobile/faqs/ https://aws.amazon.com/directconnect/ Domain Design Cost-Optimized Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
    "options": [
      "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances",
      "Use Amazon EC2 instances with Amazon EFS mount points",
      "Use Instance Store based Amazon EC2 instances",
      "Use Amazon EC2 instances with access to Amazon S3 based storage"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
    "options": [
      "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
      "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
      "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
    "options": [
      "Use Amazon EC2 reserved instances to run the workflow processes",
      "Use Amazon EC2 spot instances to run the workflow processes",
      "Use AWS Lambda function to run the workflow processes",
      "Use Amazon EC2 on-demand instances to run the workflow processes"
    ],
    "correctAnswer": 1,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A financial services company uses Amazon GuardDuty for analyzing its AWS account metadata to meet the compliance guidelines. However, the company has now decided to stop using Amazon GuardDuty service. All the existing findings have to be deleted and cannot persist anywhere on AWS Cloud. Which of the following techniques will help the company meet this requirement?",
    "options": [
      "Disable the service in the general settings",
      "Raise a service request with Amazon to completely delete the data from all their backups",
      "De-register the service under services tab",
      "Suspend the service in the general settings"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two) Create AWS account root user access keys and share those keys only with the business owner Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future Your selection is correct Create a strong password for the AWS account root user Encrypt the access keys and save them on Amazon S3 Your selection is correct Enable Multi Factor Authentication (MFA) for the AWS account root user account Overall explanation Correct options: Create a strong password for the AWS account root user Enable Multi Factor Authentication (MFA) for the AWS account root user account Here are some of the best practices while creating an AWS account root user: 1) Use a strong password to help protect account-level access to the AWS Management Console. 2) Never share your AWS account root user password or access keys with anyone. 3) If you do have an access key for your AWS account root user, delete it. If you must keep it, rotate (change) the access key regularly. You should not encrypt the access keys and save them on Amazon S3. 4) If you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. 5) Enable AWS multi-factor authentication (MFA) on your AWS account root user account. AWS Root Account Security Best Practices: via - https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html Incorrect options: Encrypt the access keys and save them on Amazon S3 - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Even an encrypted access key for the root user poses a significant security risk. Therefore, this option is incorrect. Create AWS account root user access keys and share those keys only with the business owner - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Hence, this option is incorrect. Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future - AWS recommends that you should never share your AWS account root user password or access keys with anyone. Sending an email with AWS account root user credentials creates a security risk as it can be misused by anyone reading the email. Hence, this option is incorrect. Reference: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
    "options": [
      "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
    "options": [
      "Path-based Routing",
      "HTTP header-based routing",
      "Host-based Routing",
      "Query string parameter-based routing"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected. Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
    "options": [
      "Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts",
      "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once",
      "AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs",
      "AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs"
    ],
    "correctAnswer": 1,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two) Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3 Correct selection Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket Correct selection Use multipart uploads for faster file uploads into the destination Amazon S3 bucket Your selection is incorrect Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3 Your selection is incorrect Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket Overall explanation Correct options: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Amazon S3TA takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. Use multipart uploads for faster file uploads into the destination Amazon S3 bucket Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput, therefore it facilitates faster file uploads. Incorrect options: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3 - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Direct connect takes significant time (several months) to be provisioned and is an overkill for the given use-case. Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3 - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have low to modest bandwidth requirements and can tolerate the inherent variability in Internet-based connectivity. Site-to-site VPN will not help in accelerating the file transfer speeds into S3 for the given use-case. Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. AWS Global Accelerator will not help in accelerating the file transfer speeds into S3 for the given use-case. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html Domain Design Cost-Optimized Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
    "options": [
      "Amazon S3 Standard",
      "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
      "Amazon S3 Glacier Deep Archive",
      "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)"
    ],
    "correctAnswer": 1,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
    "options": [
      "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit",
      "The engineering team needs to provision more servers running the Amazon SNS service",
      "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit",
      "The engineering team needs to provision more servers running the AWS Lambda service"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "As part of a pilot program, a biotechnology company wants to integrate data files from its on-premises analytical application with AWS Cloud via an NFS interface. Which of the following AWS service is the MOST efficient solution for the given use-case?",
    "options": [
      "AWS Site-to-Site VPN",
      "AWS Storage Gateway - Volume Gateway",
      "AWS Storage Gateway - Tape Gateway",
      "AWS Storage Gateway - File Gateway"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
    "options": [
      "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)",
      "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)",
      "Configure the security group on the Application Load Balancer",
      "Configure the security group for the Amazon EC2 instances"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes. As a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two) Amazon OpenSearch Service Amazon Redshift Your selection is correct Amazon DynamoDB Accelerator (DAX) Your selection is correct Amazon ElastiCache Amazon Relational Database Service (Amazon RDS) Overall explanation Correct options: Amazon DynamoDB Accelerator (DAX) Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option. DAX Overview: via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html Amazon ElastiCache Amazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option. Incorrect options: Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Amazon RDS cannot be used as a caching layer for Amazon DynamoDB. Amazon OpenSearch Service - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. It cannot be used as a caching layer for Amazon DynamoDB. Amazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used as a caching layer for Amazon DynamoDB. References: https://aws.amazon.com/dynamodb/dax/ https://aws.amazon.com/elasticache/faqs/ Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
    "options": [
      "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow",
      "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two) Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy Your selection is correct Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again Your selection is correct Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue Overall explanation Correct options: Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service - You can put an instance that is in the InService state into the Standby state, update some software or troubleshoot the instance, and then return the instance to service. Instances that are on standby are still part of the Auto Scaling group, but they do not actively handle application traffic. How Standby State Works: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again - The ReplaceUnhealthy process terminates instances that are marked as unhealthy and then creates new instances to replace them. Amazon EC2 Auto Scaling stops replacing instances that are marked as unhealthy. Instances that fail EC2 or Elastic Load Balancing health checks are still marked as unhealthy. As soon as you resume the ReplaceUnhealthly process, Amazon EC2 Auto Scaling replaces instances that were marked unhealthy while this process was suspended. Incorrect options: Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue - Taking the snapshot of the existing instance to create a new AMI and then creating a new instance in order to apply the maintenance patch is not time/resource optimal, hence this option is ruled out. Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy - It's not recommended to delete the Auto Scaling group just to apply a maintenance patch on a specific instance. Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again - Amazon EC2 Auto Scaling does not execute scaling actions that are scheduled to run during the suspension period. This option is not relevant to the given use-case. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/health-checks-overview.html Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
    "options": [
      "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud",
      "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud",
      "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud",
      "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
    "options": [
      "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
      "Microsoft SQL Server on AWS",
      "Amazon FSx for Lustre",
      "Amazon FSx for Windows File Server"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
    "options": [
      "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%",
      "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%",
      "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%",
      "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
    "options": [
      "Correct selection",
      "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket",
      "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager",
      "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
    "options": [
      "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
    "options": [
      "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance",
      "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance",
      "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
    "options": [
      "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
      "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
      "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
      "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service. Which of the following would you identify as data sources supported by Amazon GuardDuty?",
    "options": [
      "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs",
      "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
    "options": [
      "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used",
      "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests",
      "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
    "options": [
      "The junior scientist only needs to pay S3TA transfer charges for the image upload",
      "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload",
      "The junior scientist does not need to pay any transfer charges for the image upload",
      "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
    "options": [
      "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53",
      "Use Amazon CloudFront with a custom origin pointing to the on-premises servers",
      "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers",
      "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two) Your selection is incorrect When you use bucket default settings, you specify a Retain Until Date for the object version You cannot place a retention period on an object version through a bucket default setting Your selection is correct When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version The bucket default settings will override any explicit retention mode or period you request on an object version Correct selection Different versions of a single object can have different retention modes and periods Overall explanation Correct options: When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version You can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires. Different versions of a single object can have different retention modes and periods Like all other Object Lock settings, retention periods apply to individual object versions. Different versions of a single object can have different retention modes and periods. For example, suppose that you have an object that is 15 days into a 30-day retention period, and you PUT an object into Amazon S3 with the same name and a 60-day retention period. In this case, your PUT succeeds, and Amazon S3 creates a new version of the object with a 60-day retention period. The older version maintains its original retention period and becomes deletable in 15 days. Incorrect options: You cannot place a retention period on an object version through a bucket default setting - You can place a retention period on an object version either explicitly or through a bucket default setting. When you use bucket default settings, you specify a Retain Until Date for the object version - When you use bucket default settings, you don't specify a Retain Until Date. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected. The bucket default settings will override any explicit retention mode or period you request on an object version - If your request to place an object version in a bucket contains an explicit retention mode and period, those settings override any bucket default settings for that object version. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
    "options": [
      "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment",
      "It is not possible to access cross-account resources",
      "Both IAM roles and IAM users can be used interchangeably for cross-account access"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
    "options": [
      "Grant maximum privileges to avoid assigning privileges again",
      "Your selection is correct",
      "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions",
      "Your selection is correct"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
    "options": [
      "Only root user should have full database access in the organization",
      "Remove full database access for all IAM users in the organization",
      "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals",
      "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs the games table to be accessible globally but needs the users and games_played tables to be regional only. How would you implement this with minimal application refactoring?",
    "options": [
      "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables",
      "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables",
      "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables",
      "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created in us-east-1 region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
    "options": [
      "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection",
      "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region",
      "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region",
      "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two) Use AWS Shield Correct selection Use Amazon Aurora Replica Your selection is correct Use Amazon CloudFront distribution in front of the Application Load Balancer Use AWS Direct Connect Your selection is incorrect Use AWS Global Accelerator Overall explanation Correct options: You can use Amazon Aurora replicas and Amazon CloudFront distribution to make the application more resilient to spikes in request rates. Use Amazon Aurora Replica Amazon Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Amazon Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. Up to 15 Aurora Replicas can be distributed across the Availability Zones (AZs) that a DB cluster spans within an AWS Region. Use Amazon CloudFront distribution in front of the Application Load Balancer Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. Amazon CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Amazon CloudFront offers an origin failover feature to help support your data resiliency needs. Amazon CloudFront is a global service that delivers your content through a worldwide network of data centers called edge locations or points of presence (POPs). If your content is not already cached in an edge location, Amazon CloudFront retrieves it from an origin that you've identified as the source for the definitive version of the content. Incorrect options: Use AWS Shield - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency. There are two tiers of AWS Shield - Standard and Advanced. AWS Shield cannot be used to improve application resiliency to handle spikes in traffic. Use AWS Global Accelerator - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. Amazon Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Since Amazon CloudFront is better for improving application resiliency to handle spikes in traffic, so this option is ruled out. Use AWS Direct Connect - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. AWS Direct Connect cannot be used to improve application resiliency to handle spikes in traffic. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/disaster-recovery-resiliency.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html https://aws.amazon.com/global-accelerator/faqs/ https://docs.aws.amazon.com/global-accelerator/latest/dg/disaster-recovery-resiliency.html Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. \"Version\": \"2021-10-17\", \"Statement\": [ { \"Action\": [ \"s3:ListBucket\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket\" ], \"Effect\": \"Allow\" } ] Which statement should a solutions architect add to the policy to address this issue?",
    "options": [
      "{",
      "\"Action\": [",
      "\"s3:*\"",
      "],"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company has a web application that runs 24*7 in the production environment. The development team at the company runs a clone of the same application in the dev environment for up to 8 hours every day. The company wants to build the MOST cost-optimal solution by deploying these applications using the best-fit pricing options for Amazon Elastic Compute Cloud (Amazon EC2) instances. What would you recommend?",
    "options": [
      "Use Amazon EC2 reserved instance (RI) for the production application and spot block instances for the dev application",
      "Use on-demand Amazon EC2 instances for the production application and spot instances for the dev application",
      "Use Amazon EC2 reserved instance (RI) for the production application and spot instances for the dev application",
      "Use Amazon EC2 reserved instance (RI) for the production application and on-demand instances for the dev application"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
    "options": [
      "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate",
      "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate",
      "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages",
      "Use Amazon SQS standard queue to process the messages"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A Machine Learning research group uses a proprietary computer vision application hosted on an Amazon EC2 instance. Every time the instance needs to be stopped and started again, the application takes about 3 minutes to start as some auxiliary software programs need to be executed so that the application can function. The research group would like to minimize the application boostrap time whenever the system needs to be stopped and then started at a later point in time. As a solutions architect, which of the following solutions would you recommend for this use-case?",
    "options": [
      "Use Amazon EC2 User-Data",
      "Use Amazon EC2 Instance Hibernate",
      "Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that",
      "Use Amazon EC2 Meta-Data"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Which of the following IAM policies provides read-only access to the Amazon S3 bucket mybucket and its content? { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] } { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }",
    "options": [
      "{",
      "\"Version\":\"2012-10-17\",",
      "\"Statement\":[",
      "{"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A startup has just developed a video backup service hosted on a fleet of Amazon EC2 instances. The Amazon EC2 instances are behind an Application Load Balancer and the instances are using Amazon Elastic Block Store (Amazon EBS) Volumes for storage. The service provides authenticated users the ability to upload videos that are then saved on the EBS volume attached to a given instance. On the first day of the beta launch, users start complaining that they can see only some of the videos in their uploaded videos backup. Every time the users log into the website, they claim to see a different subset of their uploaded videos. Which of the following is the MOST optimal solution to make sure that users can view all the uploaded videos? (Select two) Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos Correct selection Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos Your selection is correct Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos Overall explanation Correct options: Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale. Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. As Amazon EBS volumes are attached locally to the Amazon EC2 instances, therefore the uploaded videos are tied to specific Amazon EC2 instances. Every time the user logs in, they are directed to a different instance and therefore their videos get dispersed across multiple EBS volumes. The correct solution is to use either Amazon S3 or Amazon EFS to store the user videos. Incorrect options: Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos - Amazon S3 Glacier Deep Archive is meant to be used for long term data archival. It cannot be used to serve static content such as videos or images via a web application. So this option is incorrect. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos - Amazon RDS is a relational database and not the right candidate for storing videos. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos - Amazon DynamoDB is a NoSQL database and not the right candidate for storing videos. Reference: https://aws.amazon.com/ebs/ Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An IT company is working on client engagement to build a real-time data analytics tool for the Internet of Things (IoT) data. The IoT data is funneled into Amazon Kinesis Data Streams which further acts as the source of a delivery stream for Amazon Kinesis Firehose. The engineering team has now configured a Kinesis Agent to send IoT data from another set of devices to the same Amazon Kinesis Firehose delivery stream. They noticed that data is not reaching Kinesis Firehose as expected. As a solutions architect, which of the following options would you attribute as the MOST plausible root cause behind this issue? The data sent by Kinesis Agent is lost because of a configuration error",
    "options": [
      "Kinesis Agent cannot write to Amazon Kinesis Firehose for which the delivery stream source is already set as Amazon Kinesis Data Streams",
      "Kinesis Agent can only write to Amazon Kinesis Data Streams, not to Amazon Kinesis Firehose"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A retail company wants to share sensitive accounting data that is stored in an Amazon RDS database instance with an external auditor. The auditor has its own AWS account and needs its own copy of the database. Which of the following would you recommend to securely share the database with the auditor?",
    "options": [
      "Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access",
      "Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket",
      "Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket",
      "Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A financial services company wants a single log processing model for all the log files (consisting of system logs, application logs, database logs, etc) that can be processed in a serverless fashion and then durably stored for downstream analytics. The company wants to use an AWS managed service that automatically scales to match the throughput of the log data and requires no ongoing administration. As a solutions architect, which of the following AWS services would you recommend solving this problem?",
    "options": [
      "Amazon EMR",
      "Amazon Kinesis Data Firehose",
      "AWS Lambda",
      "Amazon Kinesis Data Streams"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two) AWS Lambda Your selection is correct Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) AWS Step Functions Your selection is correct Amazon CloudWatch Overall explanation Correct options: Amazon Simple Notification Service (Amazon SNS) Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon CloudWatch Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. Amazon CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch Alarms to send an email via Amazon SNS whenever any of the Amazon EC2 instances breaches a certain threshold. Hence both these options are correct. Incorrect options: AWS Lambda - With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running. You can run code for virtually any type of application or backend service—all with zero administration. You cannot use AWS Lambda to monitor CPU utilization of Amazon EC2 instances or send notification emails, hence this option is incorrect. Amazon Simple Queue Service (Amazon SQS) - Amazon SQS Standard offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. You cannot use Amazon SQS to monitor CPU utilization of Amazon EC2 instances or send notification emails, hence this option is incorrect. AWS Step Functions - AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into feature-rich applications. You cannot use Step Functions to monitor CPU utilization of Amazon EC2 instances or send notification emails, hence this option is incorrect. References: https://aws.amazon.com/cloudwatch/faqs/ https://aws.amazon.com/sns/ Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A media company is migrating its flagship application from its on-premises data center to AWS for improving the application's read-scaling capability as well as its availability. The existing architecture leverages a Microsoft SQL Server database that sees a heavy read load. The engineering team does a full copy of the production database at the start of the business day to populate a dev database. During this period, application users face high latency leading to a bad user experience. The company is looking at alternate database options and migrating database engines if required. What would you suggest?",
    "options": [
      "Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database",
      "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump",
      "Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database",
      "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora"
    ],
    "correctAnswer": 3,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "What is true about Amazon RDS Read Replicas encryption?",
    "options": [
      "If the master database is encrypted, the read replicas are encrypted",
      "If the master database is unencrypted, the read replicas are encrypted",
      "If the master database is encrypted, the read replicas can be either encrypted or unencrypted",
      "If the master database is unencrypted, the read replicas can be either encrypted or unencrypted"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An IT company has built a solution wherein an Amazon Redshift cluster writes data to an Amazon S3 bucket belonging to a different AWS account. However, it is found that the files created in the Amazon S3 bucket using the UNLOAD command from the Amazon Redshift cluster are not even accessible to the Amazon S3 bucket owner. What could be the reason for this denial of permission for the bucket owner?",
    "options": [
      "By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster",
      "When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "To improve the performance and security of the application, the engineering team at a company has created an Amazon CloudFront distribution with an Application Load Balancer as the custom origin. The team has also set up an AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution. The security team at the company has noticed a surge in malicious attacks from a specific IP address to steal sensitive data stored on the Amazon EC2 instances. As a solutions architect, which of the following actions would you recommend to stop the attacks?",
    "options": [
      "Create a deny rule for the malicious IP in the Security Groups associated with each of the instances",
      "Create a ticket with AWS support to take action against the malicious IP",
      "Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances",
      "Create an IP match condition in the AWS WAF to block the malicious IP address"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two) Your selection is correct Amazon DynamoDB Your selection is correct AWS Lambda Amazon RDS Amazon Redshift Amazon ElastiCache Overall explanation Correct options: AWS Lambda With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running. You can run code for virtually any type of application or backend service—all with zero administration. Amazon DynamoDB Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Amazon DynamoDB is a NoSQL database and it's best suited to store data in key-value pairs. AWS Lambda can be combined with DynamoDB to process and capture the key-value data from the IoT sources described in the use-case. So both these options are correct. Incorrect options: Amazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. You cannot use Redshift to capture data in key-value pairs from the IoT sources, so this option is not correct. Amazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Elasticache is used as a caching layer in front of relational databases. It is not a good fit to store data in key-value pairs from the IoT sources, so this option is not correct. Amazon RDS - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Relational databases are not a good fit to store data in key-value pairs, so this option is not correct. References: https://aws.amazon.com/dynamodb/ https://aws.amazon.com/lambda/faqs/ Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A big-data consulting firm is working on a client engagement where the extract, transform, and load (ETL) workloads are currently handled via a Hadoop cluster deployed in the on-premises data center. The client wants to migrate their ETL workloads to AWS Cloud. The AWS Cloud solution needs to be highly available with about 50 Amazon Elastic Compute Cloud (Amazon EC2) instances per Availability Zone (AZ). As a solutions architect, which of the following Amazon EC2 placement groups would you recommend for handling the distributed ETL workload?",
    "options": [
      "Both Spread placement group and Partition placement group",
      "Spread placement group",
      "Partition placement group",
      "Cluster placement group"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ) us-east-1a as it has the most number of instances amongst the Availability Zone (AZs) being used currently. There are 4 instances in the Availability Zone (AZ) us-east-1a like so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour. Which of the following instances would be terminated per the default termination policy?",
    "options": [
      "Instance A",
      "Instance D",
      "Instance B",
      "Instance C"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An application is currently hosted on four Amazon EC2 instances (behind Application Load Balancer) deployed in a single Availability Zone (AZ). To maintain an acceptable level of end-user experience, the application needs at least 4 instances to be always available. As a solutions architect, which of the following would you recommend so that the application achieves high availability with MINIMUM cost?",
    "options": [
      "Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)",
      "Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ)",
      "Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)",
      "Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ)"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "What does this IAM policy do? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Mystery Policy\", \"Action\": [ \"ec2:RunInstances\" ], \"Effect\": \"Allow\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": \"eu-west-1\" } } } ] }",
    "options": [
      "It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region",
      "It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region",
      "It allows running Amazon EC2 instances anywhere but in the eu-west-1 region",
      "It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An analytics company wants to improve the performance of its big data processing workflows running on Amazon Elastic File System (Amazon EFS). Which of the following performance modes should be used for Amazon EFS to address this requirement? Provisioned Throughput General Purpose Bursting Throughput",
    "options": [
      "Max I/O"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
    "options": [
      "IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "You are establishing a monitoring solution for desktop systems, that will be sending telemetry data into AWS every 1 minute. Data for each system must be processed in order, independently, and you would like to scale the number of consumers to be possibly equal to the number of desktop systems that are being monitored. What do you recommend?",
    "options": [
      "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID",
      "Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is",
      "Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID",
      "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An application runs big data workloads on Amazon Elastic Compute Cloud (Amazon EC2) instances. The application runs 24x7 all round the year and needs at least 20 instances to maintain a minimum acceptable performance threshold and the application needs 300 instances to handle spikes in the workload. Based on historical workloads processed by the application, it needs 80 instances 80% of the time. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution so that it can meet the workload demand in a steady state?",
    "options": [
      "Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand",
      "Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "An IT company is working on a client project to build a Supply Chain Management application. The web-tier of the application runs on an Amazon EC2 instance and the database tier is on Amazon RDS MySQL. For beta testing, all the resources are currently deployed in a single Availability Zone (AZ). The development team wants to improve application availability before the go-live. Given that all end users of the web application would be located in the US, which of the following would be the MOST resource-efficient solution?",
    "options": [
      "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration",
      "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration",
      "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration",
      "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration"
    ],
    "correctAnswer": 3,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "Consider the following policy associated with an IAM group containing several users: { \"Version\":\"2012-10-17\", \"Id\":\"EC2TerminationPolicy\", \"Statement\":[ { \"Effect\":\"Deny\", \"Action\":\"ec2:*\", \"Resource\":\"*\", \"Condition\":{ \"StringNotEquals\":{ \"ec2:Region\":\"us-west-1\" } } }, { \"Effect\":\"Allow\", \"Action\":\"ec2:TerminateInstances\", \"Resource\":\"*\", \"Condition\":{ \"IpAddress\":{ \"aws:SourceIp\":\"10.200.200.0/24\" } } } ] } Which of the following options is correct?",
    "options": [
      "Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200",
      "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200",
      "Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200",
      "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "Your company has deployed an application that will perform a lot of overwrites and deletes on data and require the latest information to be available anytime data is read via queries on database tables. As a Solutions Architect, which database technology will you recommend?",
    "options": [
      "Amazon ElastiCache",
      "Amazon Simple Storage Service (Amazon S3)",
      "Amazon Neptune",
      "Amazon Relational Database Service (Amazon RDS)"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A Hollywood studio is planning a series of promotional events leading up to the launch of the trailer of its next sci-fi thriller. The executives at the studio want to create a static website with lots of animations in line with the theme of the movie. The studio has hired you as a solutions architect to build a scalable serverless solution. Which of the following represents the MOST cost-optimal and high-performance solution?",
    "options": [
      "Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin",
      "Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin",
      "Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company has recently launched a new mobile gaming application that the users are adopting rapidly. The company uses Amazon RDS MySQL as the database. The engineering team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage. As a solutions architect, which of the following solutions would you recommend so that it requires minimum development and systems administration effort to address this requirement?",
    "options": [
      "Enable storage auto-scaling for Amazon RDS MySQL",
      "Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required",
      "Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling",
      "Create read replica for Amazon RDS MySQL"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A junior DevOps engineer wants to change the default configuration for Amazon EBS volume termination. By default, the root volume of an Amazon EC2 instance for an EBS-backed AMI is deleted when the instance terminates. Which option below helps change this default behavior to ensure that the volume persists even after the instance terminates?",
    "options": [
      "Set the TerminateOnDelete attribute to false",
      "Set the DeleteOnTermination attribute to true",
      "Set the DeleteOnTermination attribute to false",
      "Set the TerminateOnDelete attribute to true"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
    "options": [
      "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda",
      "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
    "options": [
      "Activate read-through caching on the Amazon Aurora database",
      "Set up a read replica and modify the application to use the appropriate endpoint",
      "Configure the application to read from the Multi-AZ standby instance",
      "Provision another Amazon Aurora database and link it to the primary database as a read replica"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An engineering team wants to examine the feasibility of the user data feature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two) Correct selection By default, user data runs only during the boot cycle when you first launch an instance By default, user data is executed every time an Amazon EC2 instance is re-started Your selection is incorrect When an instance is running, you can update user data by using root user credentials Your selection is incorrect By default, scripts entered as user data do not have root user privileges for executing Correct selection By default, scripts entered as user data are executed with root user privileges Overall explanation Correct options: User Data is generally used to perform common automated configuration tasks and even run scripts after the instance starts. When you launch an instance in Amazon EC2, you can pass two types of user data - shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text or as a file. By default, scripts entered as user data are executed with root user privileges Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script. By default, user data runs only during the boot cycle when you first launch an instance By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. Incorrect options: By default, user data is executed every time an Amazon EC2 instance is re-started - As discussed above, this is not a default configuration of the system. But, can be achieved by explicitly configuring the instance. When an instance is running, you can update user data by using root user credentials - You can't change the user data if the instance is running (even by using root user credentials), but you can view it. By default, scripts entered as user data do not have root user privileges for executing - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination, and accesses a PostgreSQL database managed by Amazon RDS. How should you configure the security groups? (Select three) Your selection is incorrect The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432 Your selection is incorrect The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80 The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80 Correct selection The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80 Correct selection The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432 Your selection is correct The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443 Overall explanation Correct options: The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432 The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80 The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443 A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance. The following are the characteristics of security group rules: 1. By default, security groups allow all outbound traffic. 2. Security group rules are always permissive; you can't create rules that deny access. 3. Security groups are stateful PostgreSQL port = 5432 HTTP port = 80 HTTPS port = 443 The traffic goes like this : The client sends an HTTPS request to ALB on port 443. This is handled by the rule - \"The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443\" The Application Load Balancer then forwards the request to one of the Amazon EC2 instances. This is handled by the rule - \"The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80\" The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432. This is handled by the rule - \"The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432\" Incorrect options: The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80 - The client sends an HTTPS request to ALB on port 443 and not on port 80, so this is incorrect. The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432 - The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer and not from the security group of the Amazon RDS database, so this option is incorrect. The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80 - The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432 and not on port 80, so this option is incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A social media application is hosted on an Amazon EC2 fleet running behind an Application Load Balancer. The application traffic is fronted by an Amazon CloudFront distribution. The engineering team wants to decouple the user authentication process for the application, so that the application servers can just focus on the business logic. As a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?",
    "options": [
      "Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer",
      "Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution",
      "Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer",
      "Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "You would like to migrate an AWS account from an AWS Organization A to an AWS Organization B. What are the steps do to it?",
    "options": [
      "Send an invite to the new organization. Accept the invite to the new organization from the member account. Remove the member account from the old organization",
      "Send an invite to the new organization. Remove the member account from the old organization. Accept the invite to the new organization from the member account",
      "Remove the member account from the old organization. Send an invite to the member account from the new Organization. Accept the invite to the new organization from the member account",
      "Open an AWS Support ticket to ask them to migrate the account"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "What does this IAM policy do? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Mystery Policy\", \"Action\": [ \"ec2:RunInstances\" ], \"Effect\": \"Allow\", \"Resource\": \"*\", \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": \"34.50.31.0/24\" } } } ] } It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block",
    "options": [
      "It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block",
      "It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block",
      "It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines. Which of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?",
    "options": [
      "Dedicated Instances",
      "Dedicated Hosts",
      "Spot Instances",
      "On-Demand Instances"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets. As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
    "options": [
      "Use Security Groups",
      "Use Amazon S3 Bucket Policies",
      "Use Access Control Lists (ACLs)",
      "Use Identity and Access Management (IAM) policies"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A big data consulting firm needs to set up a data lake on Amazon S3 for a Health-Care client. The data lake is split in raw and refined zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based extract, transform, and load (ETL) job into the refined zone. The business analysts run ad-hoc queries only on the data in the refined zone using Amazon Athena. The team is concerned about the cost of data storage in both the raw and refined zones as the data is increasing at a rate of 1 terabyte daily in each zone. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution? (Select two) Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format Your selection is incorrect Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation Your selection is correct Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format Create an AWS Lambda function based job to delete the raw zone data after 1 day Correct selection Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation Overall explanation Correct options: Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation You can manage your objects so that they are stored cost-effectively throughout their lifecycle by configuring their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. For example, you might choose to transition objects to the Amazon S3 Standard-IA storage class 30 days after you created them, or archive objects to the Amazon S3 Glacier storage class one year after creating them. For the given use-case, the raw zone consists of the source data, so it cannot be deleted due to compliance reasons. Therefore, you should use a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation. Please read more about Amazon S3 Object Lifecycle Management: via - https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You cannot transition the refined zone data into Amazon S3 Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Therefore, the best optimization is to have the refined zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the refined zone. Please see this example for a AWS Glue ETL Pipeline: via - https://aws.amazon.com/glue/ Incorrect options: Create an AWS Lambda function based job to delete the raw zone data after 1 day - As mentioned in the use-case, the source data needs to be kept for a minimum of 5 years for compliance reasons. Therefore the data in the raw zone cannot be deleted after 1 day. Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation - You cannot transition the refined zone data into Amazon S3 Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Hence this option is incorrect. Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format - It is cost-optimal to write the data in the refined zone using a compressed format instead of CSV format. The compressed data would reduce the storage cost incurred on the data in the refined zone. So, this option is incorrect. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html https://aws.amazon.com/glue/ Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Your company has a monthly big data workload, running for about 2 hours, which can be efficiently distributed across multiple servers of various sizes, with a variable number of CPUs. The solution for the workload should be able to withstand server failures. Which is the MOST cost-optimal solution for this workload?",
    "options": [
      "Run the workload on Spot Instances",
      "Run the workload on Reserved Instances (RI)",
      "Run the workload on a Spot Fleet",
      "Run the workload on Dedicated Hosts"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
    "options": [
      "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region",
      "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases",
      "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region",
      "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key. Which of the following Amazon S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?",
    "options": [
      "Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)",
      "Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3",
      "Server-Side Encryption with Amazon S3 managed keys (SSE-S3)",
      "Server-Side Encryption with Customer-Provided Keys (SSE-C)"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An IT company wants to optimize the costs incurred on its fleet of 100 Amazon EC2 instances for the next year. Based on historical analyses, the engineering team observed that 70 of these instances handle the compute services of its flagship application and need to be always available. The other 30 instances are used to handle batch jobs that can afford a delay in processing. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution?",
    "options": [
      "Purchase 70 on-demand instances and 30 spot instances",
      "Purchase 70 reserved instances and 30 on-demand instances",
      "Purchase 70 reserved instances (RIs) and 30 spot instances",
      "Purchase 70 on-demand instances and 30 reserved instances"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A manufacturing company receives unreliable service from its data center provider because the company is located in an area prone to natural disasters. The company is not ready to fully migrate to the AWS Cloud, but it wants a failover environment on AWS in case the on-premises data center fails. The company runs web servers that connect to external vendors. The data available on AWS and on-premises must be uniform. Which of the following solutions would have the LEAST amount of downtime?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "You have a team of developers in your company, and you would like to ensure they can quickly experiment with AWS Managed Policies by attaching them to their accounts, but you would like to prevent them from doing an escalation of privileges, by granting themselves the AdministratorAccess managed policy. How should you proceed?",
    "options": [
      "For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves",
      "Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves",
      "Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy",
      "Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The engineering manager for a content management application wants to set up Amazon RDS read replicas to provide enhanced performance and read scalability. The manager wants to understand the data transfer charges while setting up Amazon RDS read replicas. Which of the following would you identify as correct regarding the data transfer charges for Amazon RDS read replicas?",
    "options": [
      "There are data transfer charges for replicating data within the same Availability Zone (AZ)",
      "There are data transfer charges for replicating data across AWS Regions",
      "There are no data transfer charges for replicating data across AWS Regions",
      "There are data transfer charges for replicating data within the same AWS Region"
    ],
    "correctAnswer": 1,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A company is developing a global healthcare application that requires the least possible latency for database read/write operations from users in several geographies across the world. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution using Amazon Aurora that offers an effective recovery point objective (RPO) of seconds and a recovery time objective (RTO) of a minute. Which of the following options would you recommend?",
    "options": [
      "Set up an Amazon Aurora Global Database cluster",
      "Set up an Amazon Aurora provisioned Database cluster",
      "Set up an Amazon Aurora multi-master Database cluster",
      "Set up an Amazon Aurora serverless Database cluster"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
    "options": [
      "You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
      "You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost"
    ],
    "correctAnswer": 1,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "You would like to store a database password in a secure place, and enable automatic rotation of that password every 90 days. What do you recommend?",
    "options": [
      "AWS Key Management Service (AWS KMS)",
      "AWS CloudHSM",
      "AWS Systems Manager Parameter Store",
      "AWS Secrets Manager"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance. As a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three) Your selection is correct The instance has failed the Elastic Load Balancing (ELB) health check status Your selection is incorrect The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG) Your selection is incorrect A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive Correct selection The instance maybe in Impaired status Correct selection The health check grace period for the instance has not expired A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks Overall explanation Correct options: The health check grace period for the instance has not expired Amazon EC2 Auto Scaling doesn't terminate an instance that came into service based on Amazon EC2 status checks and Elastic Load Balancing (ELB) health checks until the health check grace period expires. More on Health check grace period: via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period The instance maybe in Impaired status Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status. Instead, Amazon EC2 Auto Scaling waits a few minutes for the instance to recover. Amazon EC2 Auto Scaling might also delay or not terminate instances that fail to report data for status checks. This usually happens when there is insufficient data for the status check metrics in Amazon CloudWatch. The instance has failed the Elastic Load Balancing (ELB) health check status By default, Amazon EC2 Auto Scaling doesn't use the results of ELB health checks to determine an instance's health status when the group's health check configuration is set to EC2. As a result, Amazon EC2 Auto Scaling doesn't terminate instances that fail ELB health checks. If an instance's status is OutofService on the ELB console, but the instance's status is Healthy on the Amazon EC2 Auto Scaling console, confirm that the health check type is set to ELB. Incorrect options: The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG) - This is an incorrect statement. Amazon EC2 Auto Scaling terminates Spot instances when capacity is no longer available or the Spot price exceeds your maximum price. A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive - This statement is incorrect. If the configuration is updated and ASG needs more number of instances, ASG will launch new, healthy instances and does not keep unhealthy ones alive. A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks - This statement is incorrect. You can define custom health checks in Amazon EC2 Auto Scaling. When a custom health check determines that an instance is unhealthy, the check manually triggers SetInstanceHealth and then sets the instance's state to Unhealthy. Amazon EC2 Auto Scaling then terminates the unhealthy instance. References: https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance/ https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-instance-how-terminated/ Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management. Which of the following solutions would you suggest as the best fit for the given use-case?",
    "options": [
      "Use AWS Lambda authorizer for Amazon API Gateway",
      "Use AWS_IAM authorization",
      "Use Amazon Cognito Identity Pools",
      "Use Amazon Cognito User Pools"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of Amazon EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed. Which of the following options represents the best solution for the given requirements?",
    "options": [
      "Amazon Elastic File System (EFS) Standard storage class",
      "Amazon Elastic Block Store (EBS)",
      "Amazon Elastic File System (EFS) Standard–IA storage class",
      "Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
    "options": [
      "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI",
      "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company has historically operated only in the us-east-1 region and stores encrypted data in Amazon S3 using SSE-KMS. As part of enhancing its security posture as well as improving the backup and recovery architecture, the company wants to store the encrypted data in Amazon S3 that is replicated into the us-west-1 AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions. Which of the following represents the best solution to address these requirements?",
    "options": [
      "Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved. As a Solutions Architect, can you identify the Amazon Virtual Private Cloud (Amazon VPC) options to be configured in order to get the private hosted zone to work?",
    "options": [
      "Remove any overlapping namespaces for the private and public hosted zones",
      "Enable DNS hostnames and DNS resolution for private hosted zones",
      "Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "Upon a security review of your AWS account, an AWS consultant has found that a few Amazon RDS databases are unencrypted. As a Solutions Architect, what steps must be taken to encrypt the Amazon RDS databases?",
    "options": [
      "Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database",
      "Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database",
      "Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ",
      "Enable encryption on the Amazon RDS database using the AWS Console"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "You would like to use AWS Snowball to move on-premises backups into a long term archival tier on AWS. Which solution provides the MOST cost savings?",
    "options": [
      "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day",
      "Create an AWS Snowball job and target a Amazon S3 Glacier Vault",
      "Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault",
      "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "An e-commerce company operates multiple AWS accounts and has interconnected these accounts in a hub-and-spoke style using the AWS Transit Gateway. Amazon Virtual Private Cloud (Amazon VPCs) have been provisioned across these AWS accounts to facilitate network isolation. Which of the following solutions would reduce both the administrative overhead and the costs while providing shared access to services required by workloads in each of the VPCs?",
    "options": [
      "Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs)",
      "Build a shared services Amazon Virtual Private Cloud (Amazon VPC)",
      "Use Fully meshed VPC Peering connection",
      "Use VPCs connected with AWS Direct Connect"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A silicon valley based startup has a two-tier architecture using Amazon EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones (AZs) and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones (AZs). The DevOps team wants to review the security configurations of the application architecture. As a solutions architect, which of the following options would you select as the MOST secure configuration? (Select two) Correct selection For security group B: Add an inbound rule that allows traffic only from security group A on port 1433 Your selection is incorrect For security group B: Add an inbound rule that allows traffic only from all sources on port 1433 Your selection is correct For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433 For security group B: Add an inbound rule that allows traffic only from security group A on port 443 For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443 Overall explanation Correct options: For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433 For security group B: Add an inbound rule that allows traffic only from security group A on port 1433 A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance. The following are the characteristics of security group rules: By default, security groups allow all outbound traffic. Security group rules are always permissive; you can't create rules that deny access. Security groups are stateful The MOST secure configuration for the given use case is: For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433 The above rules make sure that web servers are listening for traffic on all sources on the HTTPS protocol on port 443. The web servers only allow outbound traffic to MSSQL servers in Security Group B on port 1433. For security group B: Add an inbound rule that allows traffic only from security group A on port 1433. The above rule makes sure that the MSSQL servers only accept traffic from web servers in security group A on port 1433. Therefore, both of these options are correct. Incorrect options: For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443 - As the MSSQL based database instances are listening on port 1433, therefore for security group A, the outbound rule should be added on port 443 with the destination as security group B. For security group B: Add an inbound rule that allows traffic only from all sources on port 1433 - The inbound rule should allow traffic only from security group A on port 1433. Allowing traffic from all sources will compromise security. For security group B: Add an inbound rule that allows traffic only from security group A on port 443 - The inbound rule should allow traffic only from security group A on port 1433 because the MSSQL based database instances are listening on port 1433. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "You have multiple AWS accounts within a single AWS Region managed by AWS Organizations and you would like to ensure all Amazon EC2 instances in all these accounts can communicate privately. Which of the following solutions provides the capability at the CHEAPEST cost? Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together",
    "options": [
      "Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager",
      "Create a VPC peering connection between all virtual private cloud (VPCs)",
      "Create a Private Link between all the Amazon EC2 instances"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A silicon valley based startup has a content management application with the web-tier running on Amazon EC2 instances and the database tier running on Amazon Aurora. Currently, the entire infrastructure is located in us-east-1 region. The startup has 90% of its customers in the US and Europe. The engineering team is getting reports of deteriorated application performance from customers in Europe with high application load time. As a solutions architect, which of the following would you recommend addressing these performance issues? (Select two) Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region Correct selection Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53 Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53 Your selection is correct Create Amazon Aurora read replicas in the eu-west-1 region Your selection is incorrect Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53 Overall explanation Correct options: Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53 Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Use latency based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server. As customers in Europe are facing performance issues with high application load time, you can use latency based routing to reduce the latency. Hence this is the correct option. Amazon Route 53 Routing Policy Overview: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html Create Amazon Aurora read replicas in the eu-west-1 region Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Amazon Aurora read replicas can be used to scale out reads across regions. This will improve the application performance for users in Europe. Therefore, this is also a correct option for the given use-case. Incorrect options: Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53 - Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. You cannot use geolocation routing to reduce latency, hence this option is incorrect. Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53 - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records. You cannot use failover routing to reduce latency, hence this option is incorrect. Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region - Amazon Aurora Multi-AZ enhances the availability and durability for the database, it does not help in read scaling, so it is not a correct option for the given use-case. References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html https://aws.amazon.com/blogs/aws/new-cross-region-read-replicas-for-amazon-aurora/ Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Your company has an on-premises Distributed File System Replication (DFSR) service to keep files synchronized on multiple Windows servers, and would like to migrate to AWS cloud. What do you recommend as a replacement for the DFSR?",
    "options": [
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon FSx for Lustre",
      "Amazon Simple Storage Service (Amazon S3)",
      "Amazon FSx for Windows File Server"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
    "options": [
      "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment",
      "Use AWS CodeDeploy deployment options to choose the right deployment",
      "Use Elastic Load Balancing (ELB) to distribute traffic across deployments",
      "Use Amazon Route 53 weighted routing to spread traffic across different deployments"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect. What do you recommend?",
    "options": [
      "AWS Transit Gateway",
      "AWS PrivateLink",
      "Virtual private gateway (VGW)",
      "VPC Peering Connection"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A financial services company wants to store confidential data in Amazon S3 and it needs to meet the following data security and compliance norms: Encryption key usage must be logged for auditing purposes Encryption Keys must be rotated every year The data must be encrypted at rest Which is the MOST operationally efficient solution?",
    "options": [
      "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation",
      "Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation",
      "Server-side encryption (SSE-S3) with automatic key rotation",
      "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "You would like to mount a network file system on Linux instances, where files will be stored and accessed frequently at first, and then infrequently. What solution is the MOST cost-effective? Amazon FSx for Lustre",
    "options": [
      "Amazon EFS Infrequent Access",
      "Amazon S3 Intelligent Tiering",
      "Amazon S3 Glacier Deep Archive"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack. As a solutions architect, which of the following solutions would you recommend to the team?",
    "options": [
      "Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
      "Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
      "Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
      "Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
    "options": [
      "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance",
      "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance",
      "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance",
      "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast running processes whereas other microservices need to handle slower processes. Which of these options would you identify as the right way of connecting these microservices?",
    "options": [
      "Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones",
      "Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones",
      "Add Amazon EventBridge to decouple the complex architecture",
      "Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones"
    ],
    "correctAnswer": 3,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An engineering lead is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates. Which of the following options represents the correct solution to set up internet access for the private subnets?",
    "options": [
      "Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ",
      "Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ",
      "Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ",
      "Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A DevOps engineer at an IT company just upgraded an Amazon EC2 instance type from t2.nano (0.5G of RAM, 1 vCPU) to u-12tb1.metal (12.3 TB of RAM, 448 vCPUs). How would you categorize this upgrade? This is an example of high availability This is a scale up example of horizontal scalability",
    "options": [
      "This is a scale up example of vertical scalability"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The DevOps team at a multi-national company is helping its subsidiaries standardize Amazon EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project. Which of the following would you identify as CORRECT regarding the capabilities of an Amazon Machine Image (AMI)? (Select three) Your selection is correct You can share an Amazon Machine Image (AMI) with another AWS account Your selection is correct Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot Your selection is correct You can copy an Amazon Machine Image (AMI) across AWS Regions You cannot copy an Amazon Machine Image (AMI) across AWS Regions You cannot share an Amazon Machine Image (AMI) with another AWS account Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot Overall explanation Correct options: You can copy an Amazon Machine Image (AMI) across AWS Regions You can share an Amazon Machine Image (AMI) with another AWS account Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot An Amazon Machine Image (AMI) provides the information required to launch an instance. An AMI includes the following: One or more Amazon EBS snapshots, or, for instance-store-backed AMIs, a template for the root volume of the instance. Launch permissions that control which AWS accounts can use the AMI to launch instances. A block device mapping that specifies the volumes to attach to the instance when it's launched. You can copy an AMI within or across AWS Regions using the AWS Management Console, the AWS Command Line Interface or SDKs, or the Amazon EC2 API, all of which support the CopyImage action. You can copy both Amazon EBS-backed AMIs and instance-store-backed AMIs. You can copy AMIs with encrypted snapshots and also change encryption status during the copy process. Therefore, the option - \"You can copy an AMI across AWS Regions\" - is correct. Copying AMIs across regions: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html The following table shows encryption support for various AMI-copying scenarios. While it is possible to copy an unencrypted snapshot to yield an encrypted snapshot, you cannot copy an encrypted snapshot to yield an unencrypted one. Therefore, the option - \"Copying an AMI backed by an encrypted snapshot cannot result in an unencrypted target snapshot\" is correct. via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html You can share an AMI with another AWS account. To copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either the associated Amazon EBS snapshot (for an Amazon EBS-backed AMI) or an associated S3 bucket (for an instance store-backed AMI). Therefore, the option - \"You can share an AMI with another AWS account\" - is correct. Incorrect options: You cannot copy an Amazon Machine Image (AMI) across AWS Regions You cannot share an Amazon Machine Image (AMI) with another AWS account Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot These three options contradict the details provided in the explanation above. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group. The Auto Scaling group uses a Launch Template (LT1) with \"dedicated\" instance tenancy but the VPC (V1) used by the Launch Template LT1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Template (LT2) with shared (default) instance tenancy but the VPC (V2) used by the Launch Template LT2 has the instance tenancy set to dedicated. Which of the following is correct regarding the instances launched via Launch Template LT1 and Launch Template LT2?",
    "options": [
      "The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy",
      "The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy",
      "The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy",
      "The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
    "options": [
      "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations",
      "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations",
      "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations",
      "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A data analytics company manages an application that stores user data in a Amazon DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the Amazon DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest. What do you recommend?",
    "options": [
      "Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written",
      "Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data",
      "Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written",
      "Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A leading bank has moved its IT infrastructure to AWS Cloud and they have been using Amazon EC2 Auto Scaling for their web servers. This has helped them deal with traffic spikes effectively. But, their MySQL relational database has now become a bottleneck and they urgently need a fully managed auto scaling solution for their relational database to address any unpredictable changes in the traffic. Can you identify the AWS service that is best suited for this use-case?",
    "options": [
      "Amazon DynamoDB",
      "Amazon Aurora",
      "Amazon ElastiCache",
      "Amazon Aurora Serverless"
    ],
    "correctAnswer": 3,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two) Amazon Elastic Block Store (Amazon EBS) Your selection is correct Amazon FSx for Windows File Server Amazon Elastic File System (Amazon EFS) Amazon Simple Storage Service (Amazon S3) Your selection is correct File Gateway Configuration of AWS Storage Gateway Overall explanation Correct options: Amazon FSx for Windows File Server Amazon FSx for Windows File Server is a fully managed, highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. File Gateway Configuration of AWS Storage Gateway Depending on the use case, AWS Storage Gateway provides 3 types of storage interfaces for on-premises applications: File, Volume, and Tape. The File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as Network File System (NFS) and Server Message Block (SMB). Incorrect options: Amazon Elastic File System (Amazon EFS) - Amazon EFS is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics, and concurrently-accessible storage for up to thousands of Amazon EC2 instances. Amazon EFS uses the Network File System protocol. EFS does not support SMB protocol. Amazon Elastic Block Store (Amazon EBS) - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS does not support SMB protocol. Amazon Simple Storage Service (Amazon S3) - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 provides a simple, standards-based REST web services interface that is designed to work with any Internet-development toolkit. S3 does not support SMB protocol. References: https://aws.amazon.com/fsx/windows/ https://aws.amazon.com/storagegateway/file/ Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group. As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?",
    "options": [
      "You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule",
      "You can use a security group as the custom source for the inbound rule",
      "You can use an IP address as the custom source for the inbound rule",
      "You can use an Internet Gateway ID as the custom source for the inbound rule"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
    "options": [
      "Use Auto Scaling group to provide a low latency way to distribute live sports results",
      "Use AWS Global Accelerator to provide a low latency way to distribute live sports results",
      "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results",
      "Use Amazon CloudFront to provide a low latency way to distribute live sports results"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two) Your selection is correct AWS Schema Conversion Tool (AWS SCT) AWS Glue AWS Snowball Edge Your selection is correct AWS Database Migration Service (AWS DMS) Basic Schema Copy Overall explanation Correct options: AWS Schema Conversion Tool (AWS SCT) AWS Database Migration Service (AWS DMS) AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora. Given the use-case where the CTO at the company wants to move away from license-based, expensive, legacy commercial database solutions deployed at the on-premises data center to more efficient, open-source, and cost-effective options on AWS Cloud, this is an example of heterogeneous database migrations. For such a scenario, the source and target databases engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts. That makes heterogeneous migrations a two-step process. First use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database. All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located on your on-premises environment outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS. Heterogeneous Database Migrations: via - https://aws.amazon.com/dms/ Incorrect options: AWS Snowball Edge - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and AWS Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space. AWS Snowball Edge cannot be used for database migrations. AWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Therefore, it cannot be used for database migrations. Basic Schema Copy - To quickly migrate a database schema to your target instance you can rely on the Basic Schema Copy feature of AWS Database Migration Service. Basic Schema Copy will automatically create tables and primary keys in the target instance if the target does not already contain tables with the same names. Basic Schema Copy is great for doing a test migration, or when you are migrating databases heterogeneously e.g. Oracle to MySQL or SQL Server to Oracle. Basic Schema Copy will not migrate secondary indexes, foreign keys or stored procedures. When you need to use a more customizable schema migration process (e.g. when you are migrating your production database and need to move your stored procedures and secondary database objects), you must use the AWS Schema Conversion Tool. References: https://aws.amazon.com/dms/ https://aws.amazon.com/dms/faqs/ https://aws.amazon.com/dms/schema-conversion-tool/ Domain Design Cost-Optimized Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "The database backend for a retail company's website is hosted on Amazon RDS for MySQL having a primary instance and three read replicas to support read scalability. The company has mandated that the read replicas should lag no more than 1 second behind the primary instance to provide the best possible user experience. The read replicas are falling further behind during periods of peak traffic spikes, resulting in a bad user experience as the searches produce inconsistent results. You have been hired as an AWS Certified Solutions Architect - Associate to reduce the replication lag as much as possible with minimal changes to the application code or the effort required to manage the underlying resources. Which of the following will you recommend?",
    "options": [
      "Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling",
      "Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling",
      "Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas",
      "Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve Domain Name System (DNS) queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a solutions architect, which of the following solutions can be combined to address the given use case? (Select two) Your selection is incorrect Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint Correct selection Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint Your selection is incorrect Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint Correct selection Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint Overall explanation Correct options: Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS – such as Amazon EC2 instances – and can also be used to route users to infrastructure outside of AWS. By default, Amazon Route 53 Resolver automatically answers DNS queries for local VPC domain names for Amazon EC2 instances. You can integrate DNS resolution between Resolver and DNS resolvers on your on-premises network by configuring forwarding rules. To resolve any DNS queries for resources in the AWS VPC from the on-premises network, you can create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint. Resolver Inbound Endpoint: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html To resolve DNS queries for any resources in the on-premises network from the AWS VPC, you can create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint. To conditionally forward queries, you need to create Resolver rules that specify the domain names for the DNS queries that you want to forward (such as example.com) and the IP addresses of the DNS resolvers on the on-premises network that you want to forward the queries to. Resolver Outbound Endpoint: via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html Incorrect options: Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint - DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via an inbound endpoint. Hence, this option is incorrect. Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint - Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via an outbound endpoint. Hence, this option is incorrect. Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint - There is no such thing as a universal endpoint on Amazon Route 53 Resolver. This option has been added as a distractor. References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-getting-started.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the Amazon S3 outbound data costs. As a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?",
    "options": [
      "To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application",
      "Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data",
      "Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively",
      "Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
    "options": [
      "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3",
      "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3",
      "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3",
      "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
    "options": [
      "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
      "Simple Active Directory (Simple AD)",
      "Active Directory Connector",
      "Amazon Cloud Directory"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An e-commerce company runs its web application on Amazon EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an Amazon Simple Queue Service (Amazon SQS) queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
    "options": [
      "Use a simple scaling policy based on a custom Amazon SQS queue metric",
      "Use a target tracking scaling policy based on a custom Amazon SQS queue metric",
      "Use a step scaling policy based on a custom Amazon SQS queue metric",
      "Use a scheduled scaling policy based on a custom Amazon SQS queue metric"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company recently experienced a database outage in its on-premises data center. The company now wants to migrate to a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes. Which of the following solutions meets these requirements?",
    "options": [
      "Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data",
      "Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data",
      "Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance",
      "Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3. As a solutions architect, which of the following solutions would you recommend for this use-case?",
    "options": [
      "Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket",
      "Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket",
      "Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket",
      "Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company has its application servers in the public subnet that connect to the Amazon RDS instances in the private subnet. For regular maintenance, the Amazon RDS instances need patch fixes that need to be downloaded from the internet. Considering the company uses only IPv4 addressing and is looking for a fully managed service, which of the following would you suggest as an optimal solution?",
    "options": [
      "Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables",
      "Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC",
      "Configure an Egress-only internet gateway for the resources in the private subnet of the VPC",
      "Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world to upload and download video files that can reach a maximum size of 10 gigabytes. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience. As a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?",
    "options": [
      "Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service",
      "Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face",
      "Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service",
      "Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An AWS Organization is using Service Control Policies (SCPs) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines. Which of the given scenarios are correct regarding the permissions described below? (Select three) If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action Your selection is incorrect Service control policy (SCP) affects service-linked roles Correct selection Service control policy (SCP) does not affect service-linked role Correct selection Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts Your selection is incorrect Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts Your selection is correct If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action Overall explanation Correct options: If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts Service control policy (SCP) does not affect service-linked role Service control policy (SCP) are one type of policy that can be used to manage your organization. Service control policy (SCP) offers central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines. In service control policy (SCP), you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access. You can also define conditions for when to restrict access to AWS services, resources, and API actions. These restrictions even override the administrators of member accounts in the organization. Please note the following effects on permissions vis-a-vis the service control policy (SCP): If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action. Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts. Service control policy (SCP) does not affect any service-linked role. Incorrect options: If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts Service control policy (SCP) affects service-linked roles These three options contradict the details provided in the explanation above. Reference: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The development team at a retail company wants to optimize the cost of Amazon EC2 instances. The team wants to move certain nightly batch jobs to spot instances. The team has hired you as a solutions architect to provide the initial guidance. Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three) When you cancel an active spot request, it terminates the associated instance as well Your selection is correct If a spot request is persistent, then it is opened again after your Spot Instance is interrupted If a spot request is persistent, then it is opened again after you stop the Spot Instance Your selection is correct Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated Your selection is correct When you cancel an active spot request, it does not terminate the associated instance Overall explanation Correct options: If a spot request is persistent, then it is opened again after your Spot Instance is interrupted A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances. A Spot Instance request is either one-time or persistent. If the spot request is persistent, the request is opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. How Spot requests work: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that is launched based on criteria that you specify. The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated. You can submit a Spot Fleet as a one-time request, which does not persist after the instances have been terminated. You can include On-Demand Instance requests in a Spot Fleet request. When you cancel an active spot request, it does not terminate the associated instance If your Spot Instance request is active and has an associated running Spot Instance, or your Spot Instance request is disabled and has an associated stopped Spot Instance, canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. Moreover, to cancel a persistent Spot request and terminate its Spot Instances, you must cancel the Spot request first and then terminate the Spot Instances. Incorrect options: When you cancel an active spot request, it terminates the associated instance as well - If your Spot Instance request is active and has an associated running Spot Instance, then canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. So, this option is incorrect. If a spot request is persistent, then it is opened again after you stop the Spot Instance - If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. So, this option is incorrect. Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated - As mentioned above, Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html Domain Design Cost-Optimized Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon Simple Queue Service (Amazon SQS). The development team at the company wants to minimize the costs of using Amazon SQS. As a solutions architect, which of the following options would you recommend for the given use-case?",
    "options": [
      "Use SQS message timer to retrieve messages from your Amazon SQS queues",
      "Use SQS visibility timeout to retrieve messages from your Amazon SQS queues",
      "Use SQS long polling to retrieve messages from your Amazon SQS queues",
      "Use SQS short polling to retrieve messages from your Amazon SQS queues"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "An e-commerce company is using Elastic Load Balancing (ELB) for its fleet of Amazon EC2 instances spread across two Availability Zones (AZs), with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A small business has been running its IT systems on the on-premises infrastructure but the business now plans to migrate to AWS Cloud for operational efficiencies. As a Solutions Architect, can you suggest a cost-effective serverless solution for its flagship application that has both static and dynamic content?",
    "options": [
      "Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
    "options": [
      "Use Network Address Translation (NAT) instance to access Amazon SQS",
      "Use VPN connection to access Amazon SQS",
      "Use VPC endpoint to access Amazon SQS",
      "Use Internet Gateway to access Amazon SQS"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
    "options": [
      "Use AWS Glue to replicate the data from the databases into Amazon Redshift",
      "Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift",
      "Use AWS EMR to replicate the data from the databases into Amazon Redshift",
      "Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A retail company has its flagship application running on a fleet of Amazon EC2 instances behind Elastic Load Balancing (ELB). The engineering team has been seeing recurrent issues wherein the in-flight requests from the ELB to the Amazon EC2 instances are getting dropped when an instance becomes unhealthy. Which of the following features can be used to address this issue?",
    "options": [
      "Idle Timeout",
      "Sticky Sessions",
      "Connection Draining",
      "Cross Zone load balancing"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to cope up with the traffic and application freezes on most of the pages. Which of the following is a cost-optimal solution that does not need provisioning of infrastructure?",
    "options": [
      "Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets",
      "Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture",
      "Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency",
      "Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A leading news aggregation company offers hundreds of digital products and services for customers ranging from law firms to banks to consumers. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days. As a solutions architect, which of the following AWS services provides the ability to run the billing process and auditing process on the given clickstream data in the same order?",
    "options": [
      "Amazon Kinesis Data Firehose",
      "Amazon Kinesis Data Streams",
      "Amazon Simple Queue Service (SQS)",
      "Amazon Kinesis Data Analytics"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the Amazon EC2 instances running these HPC workflows?",
    "options": [
      "Elastic Network Adapter (ENA)",
      "Elastic Network Interface (ENI)",
      "Elastic IP Address (EIP)",
      "Elastic Fabric Adapter (EFA)"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the Amazon RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity. Can you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?",
    "options": [
      "Increase the size of Amazon RDS instance",
      "Create a read replica and connect the report generation tool/application to it",
      "Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ",
      "Migrate from General Purpose SSD to magnetic storage to enhance IOPS"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The engineering team at a company is moving the static content from the company's logistics website hosted on Amazon EC2 instances to an Amazon S3 bucket. The team wants to use an Amazon CloudFront distribution to deliver the static content. The security group used by the Amazon EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to Amazon CloudFront, access to the static content should only be allowed from the aforementioned IP addresses. Which options would you combine to build a solution to meet these requirements? (Select two) Correct selection Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects Your selection is incorrect Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution Your selection is correct Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy Overall explanation Correct options: Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects When you use Amazon CloudFront with an Amazon S3 bucket as the origin, you can configure Amazon CloudFront and Amazon S3 in a way that provides the following benefits: Restricts access to the Amazon S3 bucket so that it's not publicly accessible Makes sure that viewers (users) can access the content in the bucket only through the specified Amazon CloudFront distribution—that is, prevents them from accessing the content directly from the bucket, or through an unintended CloudFront distribution. To do this, configure Amazon CloudFront to send authenticated requests to Amazon S3, and configure Amazon S3 to only allow access to authenticated requests from Amazon CloudFront. Amazon CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI). Exam Alert: Please note that AWS recommends using OAC because it supports: All Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022 Amazon S3 server-side encryption with AWS KMS (SSE-KMS) Dynamic requests (POST, PUT, etc.) to Amazon S3 OAI doesn't work for the scenarios in the preceding list, or it requires extra workarounds in those scenarios. However, you will continue to see answers enlisting OAI as the preferred option in the actual exam as it takes about 6 months/1 year for a new feature to appear in the exam. Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to your protected web application resources. You can protect the following resource types: Amazon CloudFront distribution Amazon API Gateway REST API Application Load Balancer AWS AppSync GraphQL API Amazon Cognito user pool AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response. If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions via your AWS WAF. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from. For the given use case, you should add those IP addresses that are allowed in the Amazon EC2 security group into the IP match condition. Incorrect options: Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy - You cannot associate an AWS WAF ACL with an Amazon S3 bucket policy. Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution - NACL is associated with a subnet within a VPC. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a NACL cannot be associated with a Amazon CloudFront distribution. Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution - A security group acts as a virtual firewall for your Amazon EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a security group cannot be associated with Amazon CloudFront distribution. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
    "options": [
      "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor",
      "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor",
      "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor",
      "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor"
    ],
    "correctAnswer": 3,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three) Correct selection Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue Your selection is incorrect Convert the existing standard queue into a FIFO (First-In-First-Out) queue Your selection is correct Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue Your selection is correct Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second Overall explanation Correct options: Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. By default, FIFO queues support up to 3,000 messages per second with batching, or up to 300 messages per second (300 send, receive, or delete operations per second) without batching. Therefore, using batching you can meet a throughput requirement of upto 3,000 messages per second. The name of a FIFO queue must end with the .fifo suffix. The suffix counts towards the 80-character queue name limit. To determine whether a queue is FIFO, you can check whether the queue name ends with the suffix. If you have an existing application that uses standard queues and you want to take advantage of the ordering or exactly-once processing features of FIFO queues, you need to configure the queue and your application correctly. You can't convert an existing standard queue into a FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue. Incorrect options: Convert the existing standard queue into a FIFO (First-In-First-Out) queue Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue - The name of a FIFO queue must end with the .fifo suffix. Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second - By default, FIFO queues support up to 3,000 messages per second with batching. References: https://aws.amazon.com/sqs/faqs/ https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A gaming company uses Application Load Balancers in front of Amazon EC2 instances for different services and microservices. The architecture has now become complex with too many Application Load Balancers in multiple AWS Regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations. The company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?",
    "options": [
      "Configure Elastic IPs for each of the Application Load Balancers in each Region",
      "Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer",
      "Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions",
      "Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in the us-west-2 region. The IT consultant is trying to launch an Amazon EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone (AZ) of the us-west-2 region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones (AZs) are still different. As a solutions architect, which of the following would you suggest resolving this issue?",
    "options": [
      "Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts",
      "Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts",
      "Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts",
      "Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
    "options": [
      "AWS VPN CloudHub",
      "VPC Peering connection",
      "VPC Endpoint",
      "Software VPN"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single Amazon EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes. In case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?",
    "options": [
      "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume",
      "Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected",
      "Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the Network Access Control List (Network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
    "options": [
      "IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs",
      "Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic",
      "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic",
      "Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An IT company is looking to move its on-premises infrastructure to AWS Cloud. The company has a portfolio of applications with a few of them using server bound licenses that are valid for the next year. To utilize the licenses, the CTO wants to use dedicated hosts for a one year term and then migrate the given instances to default tenancy thereafter. As a solutions architect, which of the following options would you identify as CORRECT for changing the tenancy of an instance after you have launched it? (Select two) Your selection is correct You can change the tenancy of an instance from dedicated to host You can change the tenancy of an instance from default to dedicated Your selection is incorrect You can change the tenancy of an instance from default to host You can change the tenancy of an instance from dedicated to default Correct selection You can change the tenancy of an instance from host to dedicated Overall explanation Correct options: You can change the tenancy of an instance from dedicated to host You can change the tenancy of an instance from host to dedicated Each Amazon EC2 instance that you launch into a VPC has a tenancy attribute. This attribute has the following values. via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html By default, Amazon EC2 instances run on a shared-tenancy basis. Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at the hardware level. However, Dedicated Instances may share hardware with other instances from the same AWS account that is not Dedicated Instances. A Dedicated Host is also a physical server that's dedicated to your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server. Incorrect options: You can change the tenancy of an instance from default to dedicated - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect. You can change the tenancy of an instance from dedicated to default - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect. You can change the tenancy of an instance from default to host - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html Domain Design Cost-Optimized Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "An IT company is using Amazon Simple Queue Service (Amazon SQS) queues for decoupling the various components of application architecture. As the consuming components need additional time to process Amazon Simple Queue Service (Amazon SQS) messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?",
    "options": [
      "Use delay queues to postpone the delivery of new messages to the queue for a few seconds",
      "Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds",
      "Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds",
      "Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A global pharmaceutical company wants to move most of the on-premises data into Amazon S3, Amazon Elastic File System (Amazon EFS), and Amazon FSx for Windows File Server easily, quickly, and cost-effectively. As a solutions architect, which of the following solutions would you recommend as the BEST fit to automate and accelerate online data transfers to these AWS storage services?",
    "options": [
      "Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services",
      "Use File Gateway to automate and accelerate online data transfers to the given AWS storage services",
      "Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services",
      "Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
    "options": [
      "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN",
      "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN",
      "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN",
      "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain. Which of the following solutions addresses these requirements with the minimal integration effort?",
    "options": [
      "Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies",
      "Use File Gateway of AWS Storage Gateway to create a hybrid storage solution",
      "Use Amazon FSx for Windows File Server as a shared storage solution",
      "Use Amazon Elastic File System (Amazon EFS) as a shared storage solution"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
    "options": [
      "Use Amazon EC2 on-demand instances",
      "Use Amazon EC2 dedicated instances",
      "Use Amazon EC2 reserved instances (RI)",
      "Use Amazon EC2 dedicated hosts"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A company wants to improve its gaming application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's performance metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable user data for the community of users. The leaderboard would be accessed by millions of users simultaneously. Which of the following options support the case for using Amazon ElastiCache to meet the given requirements? (Select two) Your selection is incorrect Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads Correct selection Use Amazon ElastiCache to improve the performance of compute-intensive workloads Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads Use Amazon ElastiCache to run highly complex JOIN queries Your selection is correct Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads Overall explanation Correct option: Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads Use Amazon ElastiCache to improve the performance of compute-intensive workloads Amazon ElastiCache allows you to run in-memory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. via - https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, leaderboard, and Q&A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache. Overview of Amazon ElastiCache features: via - https://aws.amazon.com/elasticache/features/ Incorrect options: Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads - As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads. Caching is not a good fit for write-heavy applications as the cache goes stale at a very fast rate. Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads - ETL workloads involve reading and transforming high-volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads. Use Amazon ElastiCache to run highly complex JOIN queries - Complex JSON queries can be run on relational databases such as Amazon RDS or Amazon Aurora. Amazon ElastiCache is not a good fit for this use case. References: https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html https://aws.amazon.com/elasticache/features/ Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an Amazon EC2 instance and needs to support up to 25,000 IOPS per volume. As a solutions architect, which of the following Amazon Elastic Block Store (Amazon EBS) volume types would you recommend for this use-case?",
    "options": [
      "Provisioned IOPS SSD (io1)",
      "Throughput Optimized HDD (st1)",
      "General Purpose SSD (gp2)",
      "Cold HDD (sc1)"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com. As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?",
    "options": [
      "Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com",
      "Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com",
      "Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com",
      "Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three) Your selection is incorrect Security Groups can be associated with a NAT gateway Your selection is incorrect NAT gateway supports port forwarding NAT gateway can be used as a bastion server Your selection is correct NAT instance can be used as a bastion server Correct selection Security Groups can be associated with a NAT instance Correct selection NAT instance supports port forwarding Overall explanation Correct options: NAT instance can be used as a bastion server Security Groups can be associated with a NAT instance NAT instance supports port forwarding A NAT instance or a NAT Gateway can be used in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet. How NAT Gateway works: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html How NAT Instance works: via - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html Please see this high-level summary of the differences between NAT instances and NAT gateways relevant to the options described in the question: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html Incorrect options: NAT gateway supports port forwarding Security Groups can be associated with a NAT gateway NAT gateway can be used as a bastion server These three options contradict the details provided in the explanation above, so these options are incorrect. Reference: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Your application is hosted by a provider on yourapp.provider.com. You would like to have your users access your application using www.your-domain.com, which you own and manage under Amazon Route 53. Which Amazon Route 53 record should you create?",
    "options": [
      "Create a CNAME record",
      "Create an A record",
      "Create a PTR record",
      "Create an Alias Record"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A global manufacturing company with facilities in the US, Europe, and Asia is designing a new distributed application to optimize its procurement workflow. The orders booked in one AWS Region should be visible to all AWS Regions in a second or less. The database should be able to facilitate failover with a short Recovery Time Objective (RTO). The uptime of the application is critical to ensure that the manufacturing processes are not impacted. As a solutions architect, which of the following will you recommend as the MOST cost-effective solution?",
    "options": [
      "Provision Amazon RDS for MySQL with a cross-Region read replica",
      "Provision Amazon RDS for PostgreSQL with a cross-Region read replica",
      "Provision Amazon Aurora Global Database",
      "Provision Amazon DynamoDB global tables"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An e-commerce company has deployed its application on several Amazon EC2 instances that are configured in a private subnet using IPv4. These Amazon EC2 instances read and write a huge volume of data to and from Amazon S3 in the same AWS region. The company has set up subnet routing to direct all the internet-bound traffic through a Network Address Translation gateway (NAT gateway). The company wants to build the most cost-optimal solution without impacting the application's ability to communicate with Amazon S3 or the internet. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
    "options": [
      "Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint",
      "Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint",
      "Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an Amazon EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a Network Address Translation (NAT) instance (N1) in the subnet S1. Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the Amazon EC2 instance E1?",
    "options": [
      "Network Address Translation (NAT) instance (N1)",
      "Route Table (R1)",
      "Internet Gateway (I1)",
      "Subnet (S1)"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A healthcare company has deployed its web application on Amazon Elastic Container Service (Amazon ECS) container instances running behind an Application Load Balancer. The website slows down when the traffic spikes and the website availability is also reduced. The development team has configured Amazon CloudWatch alarms to receive notifications whenever there is an availability constraint so the team can scale out resources. The company wants an automated solution to respond to such events. Which of the following addresses the given use case?",
    "options": [
      "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold",
      "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold",
      "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold",
      "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover Amazon EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise. As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two) Your selection is incorrect Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance Correct selection A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained Your selection is correct If your instance has a public IPv4 address, it retains the public IPv4 address after recovery Overall explanation Correct options: A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata If your instance has a public IPv4 address, it retains the public IPv4 address after recovery You can create an Amazon CloudWatch alarm to automatically recover the Amazon EC2 instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group. If your instance has a public IPv4 address, it retains the public IPv4 address after recovery. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost. Incorrect options: Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance - This is incorrect as terminated instances cannot be recovered. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained - As mentioned above, during instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost. If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery - As mentioned above, if your instance has a public IPv4 address, it retains the public IPv4 address after recovery. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible. As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?",
    "options": [
      "Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression",
      "Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression",
      "Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job",
      "Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "Which of the following AWS services provides a highly available and fault-tolerant solution to capture the clickstream events from the source and then provide a concurrent feed of the data stream to the downstream applications? Amazon Kinesis Data Analytics Amazon Simple Queue Service (Amazon SQS)",
    "options": [
      "Amazon Kinesis Data Streams"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A media streaming company is looking to migrate its on-premises infrastructure into the AWS Cloud. The engineering team is looking for a fully managed NoSQL persistent data store with in-memory caching to maintain low latency that is critical for real-time scenarios such as video streaming and interactive content. The team expects the number of concurrent users to touch up to a million so the database should be able to scale elastically. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
    "options": [
      "Amazon DocumentDB",
      "Amazon ElastiCache",
      "Amazon RDS",
      "Amazon DynamoDB"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead. Which of the following would you recommend as a scalable alternative to the current solution?",
    "options": [
      "Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing",
      "Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics",
      "Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture",
      "Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company has set up AWS Organizations to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of Amazon EC2 instances, specific IAM roles for AWS Lambda functions, etc. As a solutions architect, which of the following options would you recommend for this use-case?",
    "options": [
      "Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions",
      "Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions",
      "Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions",
      "Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three) AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions Your selection is correct Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold Your selection is incorrect Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images Correct selection By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package Your selection is correct If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code Overall explanation Correct options: By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources AWS Lambda functions always operate from an AWS-owned VPC. By default, your function has the full ability to make network requests to any public internet address — this includes access to any of the public AWS APIs. For example, your function can interact with AWS DynamoDB APIs to PutItem or Query for records. You should only enable your functions for VPC access when you need to interact with a private resource located in a private subnet. An Amazon RDS instance is a good example. Once your function is VPC-enabled, all network traffic from your function is subject to the routing rules of your VPC/Subnet. If your function needs to interact with a public resource, you will need a route through a NAT gateway in a public subnet. When to VPC-Enable an AWS Lambda Function: via - https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/ Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold Since AWS Lambda functions can scale extremely quickly, this means you should have controls in place to notify you when you have a spike in concurrency. A good idea is to deploy an Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds your threshold. You should create an AWS Budget so you can monitor costs on a daily basis. If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code You can configure your AWS Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. A function can use up to 5 layers at a time. You can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 megabytes. Incorrect options: AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions - AWS Lambda allocates compute power in proportion to the memory you allocate to your function. This means you can over-provision memory to run your functions faster and potentially reduce your costs. However, AWS recommends that you should not over provision your function time out settings. Always understand your code performance and set a function time out accordingly. Overprovisioning function timeout often results in Lambda functions running longer than expected and unexpected costs. The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package - This statement is incorrect and acts as a distractor. All the dependencies are also packaged into the single Lambda deployment package. Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images - This statement is incorrect. You can now package and deploy AWS Lambda functions as container images. References: https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/ https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/ Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created Amazon RDS database which resulted in a production outage. How can you ensure that Amazon RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?",
    "options": [
      "Attach an IAM policy to interns preventing them from creating an Amazon RDS database",
      "Store your recommendations in a custom AWS Trusted Advisor rule",
      "Use AWS CloudFormation to manage Amazon RDS databases",
      "Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes. As a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?",
    "options": [
      "Use an HTTP to HTTPS redirect",
      "Use a wildcard Secure Sockets Layer certificate (SSL certificate)",
      "Change the Elastic Load Balancing (ELB) SSL Security Policy",
      "Use Secure Sockets Layer certificate (SSL certificate) with SNI"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
    "options": [
      "Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)",
      "Amazon Kinesis with Amazon Simple Email Service (Amazon SES)",
      "Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)",
      "Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company uses Application Load Balancers in multiple AWS Regions. The Application Load Balancers receive inconsistent traffic that varies throughout the year. The engineering team at the company needs to allow the IP addresses of the Application Load Balancers in the on-premises firewall to enable connectivity. Which of the following represents the MOST scalable solution with minimal configuration changes?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company runs a popular dating website on the AWS Cloud. As a Solutions Architect, you've designed the architecture of the website to follow a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. Currently, the application uses a username and password combination to connect the AWS Lambda function to the Amazon RDS database. You would like to improve the security at the authentication level by leveraging short-lived credentials. What will you choose? (Select two) Deploy AWS Lambda in a VPC Your selection is incorrect Restrict the Amazon RDS database security group to the AWS Lambda's security group Correct selection Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL Correct selection Attach an AWS Identity and Access Management (IAM) role to AWS Lambda Your selection is incorrect Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM Overall explanation Correct options: Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL Attach an AWS Identity and Access Management (IAM) role to AWS Lambda You can authenticate to your database instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a database instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication. IAM database authentication provides the following benefits: 1. Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL). 2. You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance. 3. For applications running on Amazon EC2, you can use profile credentials specific to your Amazon EC2 instance to access your database instead of a password, for greater security. Incorrect options: AWS Systems Manager Parameter Store (aka SSM Parameter Store) provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon EC2 instance IDs, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM - Retrieving credentials from SSM is overkill for the expected solution and hence this is not a correct option. Restrict the Amazon RDS database security group to the AWS Lambda's security group Deploy AWS Lambda in a VPC This question is very tricky because all answers do indeed increase security. But the question is related to authentication mechanisms, and as such, deploying an AWS Lambda in a VPC or tightening security groups does not change the authentication layer. IAM authentication to Amazon RDS is supported, which must be achieved by attaching an IAM role the AWS Lambda function References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company has built a serverless application using Amazon API Gateway and AWS Lambda. The backend is leveraging an Amazon Aurora MySQL database. The web application was initially launched in the Americas and the company would now like to expand it to Europe, where a read-only version will be available to improve latency. You plan on deploying the Amazon API Gateway and AWS Lambda using AWS CloudFormation, but would like to have a read-only copy of your data in Europe as well. As a Solutions Architect, what do you recommend?",
    "options": [
      "Use Amazon DynamoDB Streams",
      "Create an AWS Lambda function to periodically back up and restore the Amazon Aurora database in another region",
      "Use Amazon Aurora Multi-AZ",
      "Use Amazon Aurora Read Replicas"
    ],
    "correctAnswer": 3,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
    "options": [
      "Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations",
      "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances",
      "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system. As a Solutions architect, which of the following options would you recommend as the disaster recovery strategy?",
    "options": [
      "Store the installation files in Amazon S3 for quicker retrieval",
      "Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions",
      "Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions",
      "Use Amazon EC2 user data to speed up the installation process"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "You have developed a new REST API leveraging the Amazon API Gateway, AWS Lambda and Amazon Aurora database services. Most of the workload on the website is read-heavy. The data rarely changes and it is acceptable to serve users outdated data for about 24 hours. Recently, the website has been experiencing high load and the costs incurred on the Aurora database have been very high. How can you easily reduce the costs while improving performance, with minimal changes?",
    "options": [
      "Enable Amazon API Gateway Caching",
      "Add Amazon Aurora Read Replicas",
      "Switch to using an Application Load Balancer",
      "Enable AWS Lambda In Memory Caching"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "An IT company has a large number of clients opting to build their application programming interface (API) using Docker containers. To facilitate the hosting of these containers, the company is looking at various orchestration services available with AWS. As a Solutions Architect, which of the following solutions will you suggest? (Select two) Your selection is correct Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services Your selection is correct Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services Use Amazon EMR for serverless orchestration of the containerized services Use Amazon SageMaker for serverless orchestration of the containerized services Overall explanation Correct options: Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services Building APIs with Docker containers has been gaining momentum over the years. For hosting and exposing these container-based APIs, they need a solution which supports HTTP requests routing, autoscaling, and high availability. In some cases, user authorization is also needed. For this purpose, many organizations are orchestrating their containerized services with Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS), while hosting their containers on Amazon EC2 or AWS Fargate. Then, they can add scalability and high availability with Service Auto Scaling (in Amazon ECS) or Horizontal Pod Auto Scaler (in Amazon EKS), and they expose the services through load balancers. When you use Amazon ECS as an orchestrator (with EC2 or Fargate launch type), you also have the option to expose your services with Amazon API Gateway and AWS Cloud Map instead of a load balancer. AWS Cloud Map is used for service discovery: no matter how Amazon ECS tasks scale, AWS Cloud Map service names would point to the right set of Amazon ECS tasks. Then, API Gateway HTTP APIs can be used to define API routes and point them to the corresponding AWS Cloud Map services. Incorrect options: Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services - Amazon EC2 can be used to host the container services. Amazon EC2 needs hosting and management of the instance, hence does not come under serverless solution. Fargate can be used for serverless container solutions. Use Amazon EMR for serverless orchestration of the containerized services - Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. It utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon EC2 and Amazon S3. EMR is not a docker orchestration service, as required for the use case. Use Amazon SageMaker for serverless orchestration of the containerized services - Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML. A powerful tool, SageMaker is not a docker orchestration service, as required for the use case. Reference: https://aws.amazon.com/blogs/architecture/field-notes-serverless-container-based-apis-with-amazon-ecs-and-amazon-api-gateway/ Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "The engineering team at a company is running batch workloads on AWS Cloud. The team has embedded Amazon RDS database connection strings within each web server hosting the flagship application. After failing a security audit, the team is looking at a different approach to store the database secrets securely and automatically rotate the database credentials. Which of the following solutions would you recommend to meet this requirement?",
    "options": [
      "AWS Key Management Service (KMS)",
      "AWS Systems Manager Parameter Store",
      "AWS Secrets Manager",
      "AWS Systems Manager"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as Amazon S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection. What do you recommend? (Select two) Use Egress Only Internet Gateway as a backup connection Use AWS Direct Connect connection as a backup connection Your selection is correct Use AWS Direct Connect connection as a primary connection Your selection is correct Use AWS Site-to-Site VPN as a backup connection Use AWS Site-to-Site VPN as a primary connection Overall explanation Correct options: Use AWS Direct Connect connection as a primary connection AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Use AWS Site-to-Site VPN as a backup connection AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. AWS Direct Connect as a primary connection guarantees great performance and security (as the connection is private). Using Direct Connect as a backup solution would work but probably carries a risk it would fail as well. As we don't mind going over the public internet (which is reliable, but less secure as connections are going over the public route), we should use a Site to Site VPN which offers an encrypted connection to handle failover scenarios. Incorrect options: Use Egress Only Internet Gateway as a backup connection - An Egress-Only Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances. Egress-Only Internet Gateway cannot be used to connect on-premises data centers to AWS Cloud. Use AWS Site-to-Site VPN as a primary connection - AWS Site-to-Site VPN as a primary connection is not advisable since the use of internet-based connection is only for failover scenarios, as stated in the problem. Use AWS Direct Connect connection as a backup connection - AWS Direct Connect connection is a highly secure, physical connection. It is also a costly solution and hence does not make much sense to set up the connection and keep it only as a backup. References: https://aws.amazon.com/directconnect/ https://aws.amazon.com/vpn/ Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The Auto Scaling Group spans across 2 Availability Zones (AZs). AZ-A has 3 Amazon EC2 instances and AZ-B has 4 Amazon EC2 instances. The Auto Scaling Group is about to go into a scale-in event due to the triggering of a Amazon CloudWatch alarm. What will happen under the default Auto Scaling Group configuration?",
    "options": [
      "The instance with the oldest launch template or launch configuration will be terminated in AZ-B",
      "A random instance will be terminated in AZ-B",
      "A random instance in the AZ-A will be terminated",
      "An instance in the AZ-A will be created"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes. What would you recommend as an AWS Certified Solutions Architect - Associate?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "As part of the on-premises data center migration to AWS Cloud, a company is looking at using multiple AWS Snow Family devices to move their on-premises data. Which AWS Snow Family service offers the feature of storage clustering?",
    "options": [
      "AWS Snowmobile",
      "AWS Snowmobile Storage Compute",
      "AWS Snowball Edge Compute Optimized",
      "AWS Snowcone"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "You are working as an AWS architect for a weather tracking facility. You are asked to set up a Disaster Recovery (DR) mechanism with minimum costs. In case of failure, the facility can only bear data loss of approximately 15 minutes without jeopardizing the forecasting models. As a Solutions Architect, which DR method will you suggest?",
    "options": [
      "Backup and Restore",
      "Pilot Light",
      "Multi-Site",
      "Warm Standby"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "Your company runs a web portal to match developers to clients who need their help. As a solutions architect, you've designed the architecture of the website to be fully serverless with Amazon API Gateway and AWS Lambda. The backend uses Amazon DynamoDB table. You would like to automatically congratulate your developers on important milestones, such as - their first paid contract. All the contracts are stored in Amazon DynamoDB. Which Amazon DynamoDB feature can you use to implement this functionality such that there is LEAST delay in sending automatic notifications?",
    "options": [
      "Amazon DynamoDB Streams + AWS Lambda",
      "Amazon EventBridge events + AWS Lambda",
      "Amazon Simple Queue Service (Amazon SQS) + AWS Lambda",
      "Amazon DynamoDB DAX + Amazon API Gateway"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication. As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?",
    "options": [
      "Use Elastic Load Balancing (ELB) for effective decoupling of system architecture",
      "Use Amazon EventBridge to decouple the system architecture",
      "Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture",
      "Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A ride-sharing company wants to improve the ride-tracking system that stores GPS coordinates for all rides. The engineering team at the company is looking for a NoSQL database that has single-digit millisecond latency, can scale horizontally, and is serverless, so that they can perform high-frequency lookups reliably. As a Solutions Architect, which database do you recommend for their requirements?",
    "options": [
      "Amazon DynamoDB",
      "Amazon ElastiCache",
      "Amazon Relational Database Service (Amazon RDS)",
      "Amazon Neptune"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An IT company runs a high-performance computing (HPC) workload on AWS. The workload requires high network throughput and low-latency network performance along with tightly coupled node-to-node communications. The Amazon EC2 instances are properly sized for compute and storage capacity and are launched using default options. Which of the following solutions can be used to improve the performance of the workload?",
    "options": [
      "Select dedicated instance tenancy while launching Amazon EC2 instances",
      "Select the appropriate capacity reservation while launching Amazon EC2 instances",
      "Select a cluster placement group while launching Amazon EC2 instances",
      "Select an Elastic Inference accelerator while launching Amazon EC2 instances"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A Pharmaceuticals company is looking for a simple solution to connect its VPCs and on-premises networks through a central hub. As a Solutions Architect, which of the following would you suggest as the solution that requires the LEAST operational overhead?",
    "options": [
      "Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks",
      "Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks",
      "Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks",
      "Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancing (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs. What do you recommend? (Select two) Your selection is incorrect Set the minimum capacity to 3 Set the minimum capacity to 1 Correct selection Set the minimum capacity to 2 Your selection is correct Use Reserved Instances (RIs) for the minimum capacity Use Dedicated hosts for the minimum capacity Overall explanation Correct options: Set the minimum capacity to 2 An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service. You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity. An Auto Scaling group is elastic as long as it has different values for minimum and maximum capacity. All requests to change the Auto Scaling group's desired capacity (either by manual scaling or automatic scaling) must fall within these limits. Here, even though our ASG is deployed across 3 Availability Zones (AZs), the minimum capacity to be highly available is 2. When we specify 2 as the minimum capacity, the ASG would create these 2 instances in separate Availability Zones (AZs). If demand goes up, the ASG would spin up a new instance in the third Availability Zone (AZ). Later as the demand subsides, the ASG would scale-in and the instance count would be back to 2. Use Reserved Instances (RIs) for the minimum capacity Reserved Instances (RIs) provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. Since minimum capacity will always be maintained, it is cost-effective to choose reserved instances than any other option. In case of an Availability Zone (AZ) outage, the instance in that Availability Zone (AZ) would go down however the other instance would still be available. The ASG would provision the replacement instance in the third Availability Zone (AZ) to keep the minimum count to 2. Incorrect options: Set the minimum capacity to 1 - This is not failure proof, since only one instance will be maintained consistently and this will be from only one Availability Zone (AZ). Set the minimum capacity to 3 - This is not a cost-effective option, as two instances in two different Availability Zones (AZs) are enough to make the architecture disaster-proof. Use Dedicated hosts for the minimum capacity - As there is no use-case to utilize existing per-socket, per-core, or per-VM software licenses or to run the instance on a dedicated physical host, so the option to use dedicated hosts for the minimum capacity is ruled out. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html Domain Design Cost-Optimized Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly. Which of the following will you recommend as the best-fit solution?",
    "options": [
      "Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled",
      "Set up Amazon DynamoDB table with a global secondary index",
      "Set up Amazon DynamoDB table in the on-demand capacity mode",
      "Set up Amazon DynamoDB global table in the provisioned capacity mode"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A development team has configured Elastic Load Balancing for host-based routing. The idea is to support multiple subdomains and different top-level domains. The rule *.example.com matches which of the following?",
    "options": [
      "example.com",
      "test.example.com",
      "example.test.com",
      "EXAMPLE.COM"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "You are working as a Solutions Architect for a photo processing company that has a proprietary algorithm to compress an image without any loss in quality. Because of the efficiency of the algorithm, your clients are willing to wait for a response that carries their compressed images back. You also want to process these jobs asynchronously and scale quickly, to cater to the high demand. Additionally, you also want the job to be retried in case of failures. Which combination of choices do you recommend to minimize cost and comply with the requirements? (Select two) Amazon EC2 Reserved Instances (RIs) Your selection is correct Amazon Simple Queue Service (Amazon SQS) Amazon Simple Notification Service (Amazon SNS) Correct selection Amazon EC2 Spot Instances Your selection is incorrect Amazon EC2 On-Demand Instances Overall explanation Correct options: Amazon EC2 Spot Instances A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone (AZ) is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. To process these jobs, due to the unpredictable nature of their volume, and the desire to save on costs, spot Instances are recommended as compared to on-demand instances. As spot instances are cheaper than reserved instances and do not require long term commitment, spot instances are a better fit for the given use-case. Amazon EC2 Instance purchasing options: via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html Amazon Simple Queue Service (Amazon SQS) Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO (First-In-First-out) queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. Amazon SQS will allow you to buffer the image compression requests and process them asynchronously. It also has a direct built-in mechanism for retries and scales seamlessly. Incorrect options: Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS is not the right fit for this use-case, since its not a queuing mechanism. Amazon EC2 Reserved Instances (RIs) - Reserved instances (RIs) reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years. For the given use case, this kind of annual commitment might not be a desirable option. Amazon EC2 On-Demand Instances - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. There is no long-term commitment required when you purchase On-Demand Instances. You pay only for the seconds that your On-Demand Instances are running. AWS recommends that you use On-Demand Instances for applications with short-term, irregular workloads that cannot be interrupted. References: https://aws.amazon.com/sqs/ https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html Domain Design Cost-Optimized Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A photo hosting service publishes a collection of beautiful mountain images, every month, that aggregate over 50 gigabytes in size and downloaded all around the world. The content is currently hosted on Amazon EFS and distributed by Elastic Load Balancing (ELB) and Amazon EC2 instances. The website is experiencing high load each month and very high network costs. As a Solutions Architect, what can you recommend that won't force an application refactor and reduce network costs and Amazon EC2 load drastically?",
    "options": [
      "Enable Elastic Load Balancing (ELB) caching",
      "Upgrade the Amazon EC2 instances",
      "Host the master pack onto Amazon S3 for faster access",
      "Create an Amazon CloudFront distribution"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
    "options": [
      "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams",
      "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams",
      "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
    "options": [
      "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database",
      "Use IAM authentication to access the database instead of the database user's access credentials",
      "Configure Amazon RDS to use SSL for data in transit",
      "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An enterprise has decided to move its secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current tape backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move this data to cloud. What is an optimal solution that meets these requirements while keeping the costs to a minimum?",
    "options": [
      "Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity. Which of the following options will maximize the VPN throughput?",
    "options": [
      "Use Transfer Acceleration for the VPN connection to maximize the throughput",
      "Create a virtual private gateway with equal cost multipath routing and multiple channels",
      "Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels",
      "Use AWS Global Accelerator for the VPN connection to maximize the throughput"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company's business logic is built on several microservices that are running in the on-premises data center. They currently communicate using a message broker that supports the MQTT protocol. The company is looking at migrating these applications and the message broker to AWS Cloud without changing the application logic. Which technology allows you to get a managed message broker that supports the MQTT protocol?",
    "options": [
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon MQ",
      "Amazon Kinesis Data Streams"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company wants to grant access to an Amazon S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?",
    "options": [
      "Use a bucket policy to grant permission to users in its account as well as to users in another account",
      "Use permissions boundary to grant permission to users in its account as well as to users in another account",
      "Use a user policy to grant permission to users in its account as well as to users in another account",
      "Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy. As a Solutions Architect, which of the following would you identify as the correct description for the given policy?",
    "options": [
      "{",
      "\"Version\": \"2012-10-17\",",
      "\"Id\": \"S3PolicyId1\",",
      "\"Statement\": ["
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A social media company wants the capability to dynamically alter the size of a geographic area from which traffic is routed to a specific server resource. Which feature of Amazon Route 53 can help achieve this functionality?",
    "options": [
      "Latency-based routing",
      "Weighted routing",
      "Geoproximity routing",
      "Geolocation routing"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two) Store the installation files in Amazon S3 so they can be quickly retrieved Your selection is incorrect Use AWS Elastic Beanstalk deployment caching feature Use Amazon EC2 user data to install the application at boot time Your selection is correct Create a Golden Amazon Machine Image (AMI) with the static installation components already setup Correct selection Use Amazon EC2 user data to customize the dynamic installation parts at boot time Overall explanation Correct options: AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. When you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn't included in the standard AMIs. Create a Golden Amazon Machine Image (AMI) with the static installation components already setup A Golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can have the static installation components already setup via the golden AMI. Use Amazon EC2 user data to customize the dynamic installation parts at boot time Amazon EC2 instance user data is the data that you specified in the form of a configuration script while launching your instance. You can use Amazon EC2 user data to customize the dynamic installation parts at boot time, rather than installing the application itself at boot time. Incorrect options: Store the installation files in Amazon S3 so they can be quickly retrieved - Amazon S3 bucket can be used as a storage location for your source code, logs, and other artifacts that are created when you use AWS Elastic Beanstalk. It cannot be used to run or generate dynamic files since Amazon S3 is not an environment but a storage service. Use Amazon EC2 user data to install the application at boot time - User data of an instance can be used to perform common automated configuration tasks or run scripts after the instance starts. User data, cannot, however, be used to install the application since it takes over 45 minutes for the installation which contains static as well as dynamic files that must be generated during the installation process. Use AWS Elastic Beanstalk deployment caching feature - AWS Elastic Beanstalk deployment caching is a made-up option. It is just added as a distractor. References: https://aws.amazon.com/elasticbeanstalk/ https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/ https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.S3.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A digital media company needs to manage uploads of around 1 terabyte each from an application being used by a partner company. As a Solutions Architect, how will you handle the upload of these files to Amazon S3?",
    "options": [
      "Use AWS Direct Connect to provide extra bandwidth",
      "Use multi-part upload feature of Amazon S3",
      "Use AWS Snowball",
      "Use Amazon S3 Versioning"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves player’s scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
    "options": [
      "Setup Amazon ElastiCache in front of Amazon RDS",
      "Setup Amazon RDS Read Replicas",
      "Move to Amazon Redshift",
      "Switch application code to AWS Lambda for better performance"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two) Your selection is incorrect The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted You need to attach elastic IP address (EIP) to the Amazon EC2 instances Your web-app has a runtime that is not supported by the Application Load Balancer Your selection is correct The route for the health check is misconfigured Correct selection The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer Overall explanation Correct options: The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer The route for the health check is misconfigured An Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called health checks. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones (AZs) for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. If a target group contains only unhealthy registered targets, the load balancer nodes route requests across its unhealthy targets. You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions. Application Load Balancer Configuration for Security Groups and Health Check Routes: via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html Incorrect options: The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted - You can access the website using the IP address which means there is no issue with the Amazon EBS volumes. So this option is not correct. Your web-app has a runtime that is not supported by the Application Load Balancer - There is no connection between a web app runtime and the application load balancer. This option has been added as a distractor. You need to attach elastic IP address (EIP) to the Amazon EC2 instances - This option is a distractor as Elastic IPs do not need to be assigned to Amazon EC2 instances while using an Application Load Balancer. References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The engineering team at a leading e-commerce company is anticipating a surge in the traffic because of a flash sale planned for the weekend. You have estimated the web traffic to be 10x. The content of your website is highly dynamic and changes very often. As a Solutions Architect, which of the following options would you recommend to make sure your infrastructure scales for that day?",
    "options": [
      "Use an Amazon Route 53 Multi Value record",
      "Use an Auto Scaling Group",
      "Use an Amazon CloudFront distribution in front of your website",
      "Deploy the website on Amazon S3"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
    "options": [
      "Amazon Aurora",
      "Amazon Redshift",
      "Amazon OpenSearch Service",
      "Amazon Neptune"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in the us-west-1 Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in the us-east-1 Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two) Your selection is incorrect Use AWS Snowball Edge device to copy the data from one Region to another Region Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console Correct selection Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration Correct selection Copy data from the source bucket to the destination bucket using the aws S3 sync command Your selection is incorrect Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console Overall explanation Correct options: Copy data from the source bucket to the destination bucket using the aws S3 sync command The aws S3 sync command uses the CopyObject APIs to copy objects between Amazon S3 buckets. The sync command lists the source and target buckets to identify objects that are in the source bucket but that aren't in the target bucket. The command also identifies objects in the source bucket that have different LastModified dates than the objects that are in the target bucket. The sync command on a versioned bucket copies only the current version of the object—previous versions aren't copied. By default, this preserves object metadata, but the access control lists (ACLs) are set to FULL_CONTROL for your AWS account, which removes any additional ACLs. If the operation fails, you can run the sync command again without duplicating previously copied objects. You can use the command like so: aws s3 sync s3://DOC-EXAMPLE-BUCKET-SOURCE s3://DOC-EXAMPLE-BUCKET-TARGET Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration Amazon S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication. This is done through the use of a Batch Operations job. You should note that batch replication differs from live replication which continuously and automatically replicates new objects across Amazon S3 buckets. You cannot directly use the AWS S3 console to configure cross-Region replication for existing objects. By default, replication only supports copying new Amazon S3 objects after it is enabled using the AWS S3 console. Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. Object may be replicated to a single destination bucket or multiple destination buckets. Destination buckets can be in different AWS Regions or within the same Region as the source bucket. Once done, you can delete the replication configuration, as it ensures that batch replication is only used for this one-time data copy operation. If you want to enable live replication for existing objects for your bucket, you must contact AWS Support and raise a support ticket. This is required to ensure that replication is configured correctly. Incorrect options: Use AWS Snowball Edge device to copy the data from one Region to another Region - As the given requirement is about copying the data from one AWS Region to another AWS Region, so AWS Snowball Edge cannot be used here. AWS Snowball Edge Storage Optimized is the optimal data transfer choice if you need to securely and quickly transfer terabytes to petabytes of data to AWS. You can use AWS Snowball Edge Storage Optimized if you have a large backlog of data to transfer or if you frequently collect data that needs to be transferred to AWS and your storage is in an area where high-bandwidth internet connections are not available or cost-prohibitive. AWS Snowball Edge can operate in remote locations or harsh operating environments, such as factory floors, oil and gas rigs, mining sites, hospitals, and on moving vehicles. Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console - AWS S3 console cannot be used to copy 1 petabytes of data from one bucket to another as it's not feasible. You should note that this option is different from using the replication options on the AWS console, since here you are using the copy and paste options provided on the AWS console, which is suggested for small or medium data volume. You should use S3 sync for the requirement of one-time copy of data. Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console - Amazon S3 Transfer Acceleration (Amazon S3TA) is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an Amazon S3 bucket. You cannot use Transfer Acceleration to copy objects across Amazon S3 buckets in different Regions using Amazon S3 console. References: https://aws.amazon.com/premiumsupport/knowledge-center/move-objects-s3-bucket/ https://aws.amazon.com/snowball/faqs/ https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A small rental company had 5 employees, all working under the same AWS cloud account. These employees deployed their applications built for various functions- including billing, operations, finance, etc. Each of these employees has been operating in their own VPC. Now, there is a need to connect these VPCs so that the applications can communicate with each other. Which of the following is the MOST cost-effective solution for this use-case?",
    "options": [
      "Use a VPC peering connection",
      "Use an Internet Gateway",
      "Use a Network Address Translation gateway (NAT gateway)",
      "Use an AWS Direct Connect connection"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company has noticed that its Amazon EBS Elastic Volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the Amazon EC2 instance. The Amazon CloudWatch metrics report that both the Amazon EC2 instance and the Amazon EBS volume are under-utilized. The Amazon CloudWatch metrics also show that the Amazon EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation. As a Solutions Architect, what do you propose to reduce the costs?",
    "options": [
      "Convert the Amazon EC2 instance EBS volume to gp2",
      "Keep the Amazon EBS volume to io1 and reduce the IOPS",
      "Change the Amazon EC2 instance type to something much smaller",
      "Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "Amazon Route 53 is configured to route traffic to two Network Load Balancer nodes belonging to two Availability Zones (AZs): AZ-A and AZ-B. Cross-zone load balancing is disabled. AZ-A has four targets and AZ-B has six targets. Which of the below statements is true about traffic distribution to the target instances from Amazon Route 53?",
    "options": [
      "Each of the four targets in AZ-A receives 8% of the traffic",
      "Each of the four targets in AZ-A receives 12.5% of the traffic",
      "Each of the six targets in AZ-B receives 10% of the traffic",
      "Each of the four targets in AZ-A receives 10% of the traffic"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company has migrated its application from a monolith architecture to a microservices based architecture. The development team has updated the Amazon Route 53 simple record to point \"myapp.mydomain.com\" from the old Load Balancer to the new one. The users are still not redirected to the new Load Balancer. What has gone wrong in the configuration?",
    "options": [
      "The Time To Live (TTL) is still in effect",
      "The health checks are failing",
      "The Alias Record is misconfigured",
      "The CNAME Record is misconfigured"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "You have an Amazon S3 bucket that contains files in two different folders - s3://my-bucket/images and s3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole Availability Zone (AZ). How can you implement an efficient cost strategy for your Amazon S3 bucket? (Select two) Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days Correct selection Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days Your selection is correct Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days Your selection is incorrect Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days Overall explanation Correct options: To manage your S3 objects, so they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions — Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them. Expiration actions — Define when objects expire. Amazon S3 deletes expired objects on your behalf. Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per gigabyte storage price and per gigabyte retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As the use-case mentions that after 45 days, image files are rarely requested, but the thumbnails still are. So you need to use a prefix while configuring the Lifecycle Policy so that only objects in the s3://my-bucket/images are transitioned to Standard IA and not all the objects in the bucket. Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Incorrect options: Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days - As discussed above, you need to use a prefix while configuring the Lifecycle Policy so that only objects in the s3://my-bucket/images are transitioned to Amazon S3 Standard IA and not all the objects in the bucket. Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days - After 180 days, you can move all the objects to Amazon S3 Glacier storage as per the use case. Glacier doesn't need prefixes for the given use-case. Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. The minimum storage duration charge is 30 days. Finally, Amazon S3 One Zone IA will not achieve the necessary availability in case an Availability Zone (AZ) goes down. References: https://aws.amazon.com/s3/storage-classes/ https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html Domain Design Cost-Optimized Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "You are working for a software as a service (SaaS) company as a solutions architect and help design solutions for the company's customers. One of the customers is a bank and has a requirement to whitelist a public IP when the bank is accessing external services across the internet. Which architectural choice do you recommend to maintain high availability, support scaling-up to 10 instances and comply with the bank's requirements?",
    "options": [
      "Use an Application Load Balancer with an Auto Scaling Group",
      "Use an Auto Scaling Group with Dynamic Elastic IPs attachment",
      "Use a Network Load Balancer with an Auto Scaling Group",
      "Use a Classic Load Balancer with an Auto Scaling Group"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
    "options": [
      "Use a Spread placement group",
      "Optimize the Amazon EC2 kernel using EC2 User Data",
      "Use Spot Instances",
      "Use a Cluster placement group"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company has developed a popular photo-sharing website using a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. The website is experiencing high read traffic and the AWS Lambda functions are putting an increased read load on the Amazon RDS database. The architecture team is planning to increase the read throughput of the database, without changing the application's core logic. As a Solutions Architect, what do you recommend?",
    "options": [
      "Use Amazon ElastiCache",
      "Use Amazon RDS Read Replicas",
      "Use Amazon DynamoDB",
      "Use Amazon RDS Multi-AZ feature"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "What does this AWS CloudFormation snippet do? (Select three) SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0 - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: 192.168.1.1/32 It configures the inbound rules of a network access control list (network ACL) It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1 Your selection is correct It allows any IP to pass through on the HTTP port Your selection is correct It lets traffic flow from one IP on port 22 It configures a security group's outbound rules Your selection is correct It configures a security group's inbound rules It only allows the IP 0.0.0.0 to reach HTTP Overall explanation Correct options: It allows any IP to pass through on the HTTP port It configures a security group's inbound rules It lets traffic flow from one IP on port 22 A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance. The following are the characteristics of security group rules: 1. By default, security groups allow all outbound traffic. 2. Security group rules are always permissive; you can't create rules that deny access. 3. Security groups are stateful AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This gives you a single source of truth for your AWS and third-party resources. Considering the given AWS CloudFormation snippet, 0.0.0.0/0 means any IP, not the IP 0.0.0.0. Ingress means traffic going into your instance, and Security Groups are different from NACL. Each \"-\" in our security group rule represents a different rule (YAML syntax) Therefore the AWS CloudFormation snippet creates two Security Group inbound rules that allow any IP to pass through on the HTTP port and lets traffic flow from one source IP (192.168.1.1) on port 22. Incorrect options: It configures the inbound rules of a network access control list (network ACL) - A Network Access Control List ( Network ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups to add an additional layer of security to your VPC. It only allows the IP 0.0.0.0 to reach HTTP It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1 It configures a security group's outbound rules These three options contradict the description provided above. So these are incorrect. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html https://aws.amazon.com/cloudformation/ Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The engineering team at a social media company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating Amazon CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Solutions Architect – Associate to advise on Amazon CloudFront capabilities on routing, security, and high availability. Which of the following would you identify as correct regarding Amazon CloudFront? (Select three) Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content Amazon CloudFront can route to multiple origins based on the price class Use geo restriction to configure Amazon CloudFront for high-availability and failover Your selection is correct Use field level encryption in Amazon CloudFront to protect sensitive data for specific content Your selection is correct Amazon CloudFront can route to multiple origins based on the content type Your selection is correct Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover Overall explanation Correct options: Amazon CloudFront can route to multiple origins based on the content type You can configure a single Amazon CloudFront web distribution to serve different types of requests from multiple origins. For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a Amazon CloudFront web distribution. Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover You can set up Amazon CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin. To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group. via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html Use field level encryption in Amazon CloudFront to protect sensitive data for specific content Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the data—and have the credentials to decrypt it—are able to do so. To use field-level encryption, when you configure your Amazon CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request. (You can’t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt.) via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html Incorrect options: Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content - This option has been added as a distractor. You can use field level encryption in Amazon CloudFront to protect sensitive data for specific content. Use geo restriction to configure Amazon CloudFront for high-availability and failover - You can use geo restriction, also known as geo blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront distribution. Geo restriction is not used to configure Amazon CloudFront for high availability and failover. Amazon CloudFront can route to multiple origins based on the price class - Amazon CloudFront edge locations are grouped into geographic regions, and AWS has grouped regions into price classes. The default price class includes all regions. Another price class includes most regions (the United States; Canada; Europe; Hong Kong, Philippines, South Korea, Taiwan, and Singapore; Japan; India; South Africa; and Middle East regions) but excludes the most expensive regions. A third price class includes only the least expensive regions (the United States, Canada, and Europe regions). CloudFront can only route to multiple origins based on content type and not on the basis of the price class. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A niche social media application allows users to connect with sports athletes. As a solutions architect, you've designed the architecture of the application to be fully serverless using Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. Some of the star athletes using the application are highly popular, and therefore Amazon DynamoDB has increased the read capacity units (RCUs). Still, the application is experiencing a hot partition problem. What can you do to improve the performance of Amazon DynamoDB and eliminate the hot partition problem without a lot of application refactoring?",
    "options": [
      "Use Amazon DynamoDB DAX",
      "Use Amazon DynamoDB Global Tables",
      "Use Amazon ElastiCache",
      "Use Amazon DynamoDB Streams"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A music-sharing company uses a Network Load Balancer to direct traffic to 5 Amazon EC2 instances managed by an Auto Scaling group. When a very popular song is released, the Auto Scaling Group scales to 100 instances and the company incurs high network and compute fees. The company wants a solution to reduce the costs without changing any of the application code. What do you recommend?",
    "options": [
      "Move the songs to Amazon S3 Glacier",
      "Leverage AWS Storage Gateway",
      "Use an Amazon CloudFront distribution",
      "Move the songs to Amazon S3"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A developer in your company has set up a classic 2 tier architecture consisting of an Application Load Balancer and an Auto Scaling group (ASG) managing a fleet of Amazon EC2 instances. The Application Load Balancer is deployed in a subnet of size 10.0.1.0/24 and the Auto Scaling group is deployed in a subnet of size 10.0.4.0/22. As a solutions architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Amazon EC2 instances to only allow traffic coming from the Application Load Balancer?",
    "options": [
      "Add a rule to authorize the CIDR 10.0.4.0/22",
      "Add a rule to authorize the security group of the Auto Scaling group",
      "Add a rule to authorize the CIDR 10.0.1.0/24",
      "Add a rule to authorize the security group of the Application Load Balancer"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies. Which is the only resource-based policy that the IAM service supports?",
    "options": [
      "Trust policy",
      "Permissions boundary",
      "Access control list (ACL)",
      "AWS Organizations Service Control Policies (SCP)"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
    "options": [
      "Use AWS CloudTrail to analyze API calls",
      "Implement an IAM policy to forbid users to change Amazon S3 bucket settings",
      "Use Amazon S3 access logs to analyze user access using Athena",
      "Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A healthcare company is evaluating storage options on Amazon S3 to meet regulatory guidelines. The data should be stored in such a way on Amazon S3 that it cannot be deleted until the regulatory time period has expired. As a solutions architect, which of the following would you recommend for the given requirement?",
    "options": [
      "Use Amazon S3 cross-region replication (S3 CRR)",
      "Use Amazon S3 Glacier Vault Lock",
      "Activate AWS Multi-Factor Authentication (AWS MFA) delete on the Amazon S3 bucket",
      "Use Amazon S3 Object Lock"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "As a Solutions Architect, you are tasked to design a distributed application that will run on various Amazon EC2 instances. This application needs to have the highest performance local disk to cache data. Also, data is copied through an Amazon EC2 to EC2 replication mechanism. It is acceptable if the instance loses its data when stopped or terminated. Which storage solution do you recommend?",
    "options": [
      "Amazon Elastic Block Store (EBS)",
      "Instance Store",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon Simple Storage Service (Amazon S3)"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A leading e-commerce company runs its IT infrastructure on AWS Cloud. The company has a batch job running at 7AM daily on an Amazon RDS database. It processes shipping orders for the past day, and usually gets around 2000 records that need to be processed sequentially in a batch job via a shell script. The processing of each record takes about 3 seconds. What platform do you recommend to run this batch job?",
    "options": [
      "AWS Lambda",
      "Amazon Kinesis Data Streams",
      "Amazon Elastic Compute Cloud (Amazon EC2)",
      "AWS Glue"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two) Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) Amazon Kinesis Your selection is correct Amazon S3 Your selection is correct Amazon DynamoDB Overall explanation Correct options: Amazon S3 Amazon DynamoDB A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic. There are two types of VPC endpoints: Interface Endpoints and Gateway Endpoints. An Interface Endpoint is an Elastic Network Interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service. A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported: Amazon S3 and Amazon DynamoDB. You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints. A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway. You must remember that these two services use a VPC gateway endpoint. The rest of the AWS services use VPC interface endpoints. Gateway VPC endpoints: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html Incorrect options: Amazon Simple Queue Service (Amazon SQS) Amazon Simple Notification Service (Amazon SNS) Amazon Kinesis As mentioned in the description above, these three options use interface endpoints, so these are incorrect. Reference: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A media company uses Amazon ElastiCache Redis to enhance the performance of its Amazon RDS database layer. The company wants a robust disaster recovery strategy for its caching layer that guarantees minimal downtime as well as minimal data loss while ensuring good application performance. Which of the following solutions will you recommend to address the given use-case?",
    "options": [
      "Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure",
      "Schedule manual backups using Redis append-only file (AOF)",
      "Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure",
      "Schedule daily automatic backups at a time when you expect low resource utilization for your cluster"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An e-commerce company wants to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database. The solution should result in the maximum possible availability for the database layer while minimizing operational and management overhead. As a solutions architect, which of the following would you recommend to meet the given requirements?",
    "options": [
      "Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment",
      "Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration",
      "Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment",
      "Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company has recently created a new department to handle their services workload. An IT team has been asked to create a custom VPC to isolate the resources created in this new department. They have set up the public subnet and internet gateway (IGW). However, they are not able to ping the Amazon EC2 instances with elastic IP address (EIP) launched in the newly created VPC. As a Solutions Architect, the team has requested your help. How will you troubleshoot this scenario? (Select two) Contact AWS support to map your VPC with subnet Your selection is correct Check if the security groups allow ping from the source Your selection is correct Check if the route table is configured with internet gateway Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables Disable Source / Destination check on the Amazon EC2 instance Overall explanation Correct options: Check if the route table is configured with internet gateway An internet gateway (IGW) is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. An internet gateway supports IPv4 and IPv6 traffic. To enable access to or from the internet for instances in a subnet in a VPC, you must do the following: 1. Attach an internet gateway to your VPC. 2. Add a route to your subnet's route table that directs internet-bound traffic to the internet gateway. 3. Ensure that instances in your subnet have a globally unique IP address 4. Ensure that your network access control lists and security group rules allow the relevant traffic to flow to and from your instance. A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. After creating an IGW, make sure the route tables are updated. Additionally, ensure the security group allows the ICMP protocol for ping requests. Check if the security groups allow ping from the source A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, AWS uses the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. To decide whether to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated. The following are the characteristics of security group rules: 1. By default, security groups allow all outbound traffic. 2. Security group rules are always permissive; you can't create rules that deny access. 3. Security groups are stateful Incorrect options: Disable Source / Destination check on the Amazon EC2 instance - The Source/Destination Check attribute controls whether source/destination checking is enabled on the instance. Disabling this attribute enables an instance to handle network traffic that isn't specifically destined for the instance. For example, instances running services such as network address translation, routing, or a firewall should set this value to disabled. The default value is enabled. Source/Destination Check is not relevant to the question and it has been added as a distractor. Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables - There is no such thing as a secondary IGW. This option is added as a distractor. Contact AWS support to map your VPC with subnet - You cannot contact AWS support to map your VPC with the subnet. References: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#change_source_dest_check Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A financial services firm has traditionally operated with an on-premise data center and would like to create a disaster recovery strategy leveraging the AWS Cloud. As a Solutions Architect, you would like to ensure that a scaled-down version of a fully functional environment is always running in the AWS cloud, and in case of a disaster, the recovery time is kept to a minimum. Which disaster recovery strategy is that?",
    "options": [
      "Backup and Restore",
      "Warm Standby",
      "Pilot Light",
      "Multi Site"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A solutions architect has been tasked to design a low-latency solution for a static, single-page application, accessed by users through a custom domain name. The solution must be serverless, provide in-transit data encryption and needs to be cost-effective. Which AWS services can be combined to build the simplest possible solution for the company's requirement?",
    "options": [
      "Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application",
      "Host the application on Amazon EC2 instance with instance store volume for high performance and low latency access to users",
      "Host the application on AWS Fargate and front it with Elastic Load Balancing for an improved performance",
      "Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low latency access"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A cyber security company is running a mission critical application using a single Spread placement group of Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance. How many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given use-case?",
    "options": [
      "7",
      "14",
      "3",
      "15"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A junior developer is learning to build websites using HTML, CSS, and JavaScript. He has created a static website and then deployed it on Amazon S3. Now he can't seem to figure out the endpoint for his super cool website. As a solutions architect, can you help him figure out the allowed formats for the Amazon S3 website endpoints? (Select two) Your selection is correct http://bucket-name.s3-website-Region.amazonaws.com http://bucket-name.Region.s3-website.amazonaws.com http://s3-website-Region.bucket-name.amazonaws.com Your selection is incorrect http://s3-website.Region.bucket-name.amazonaws.com Correct selection http://bucket-name.s3-website.Region.amazonaws.com Overall explanation Correct options: http://bucket-name.s3-website.Region.amazonaws.com http://bucket-name.s3-website-Region.amazonaws.com To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document. Depending on your website requirements, you can also configure other options, including redirects, web traffic logging, and custom error documents. When you configure your bucket as a static website, the website is available at the AWS Region-specific website endpoint of the bucket. Depending on your Region, your Amazon S3 website endpoints follow one of these two formats. s3-website dash (-) Region ‐ http://bucket-name.s3-website.Region.amazonaws.com s3-website dot (.) Region ‐ http://bucket-name.s3-website-Region.amazonaws.com These URLs return the default index document that you configure for the website. Incorrect options: http://s3-website-Region.bucket-name.amazonaws.com http://s3-website.Region.bucket-name.amazonaws.com http://bucket-name.Region.s3-website.amazonaws.com These three options do not meet the specifications for the Amazon S3 website endpoints format, so these are incorrect. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
    "options": [
      "Establish VPC peering connections between all VPCs",
      "Use an internet gateway to interconnect the VPCs",
      "Use AWS transit gateway to interconnect the VPCs",
      "Use a VPC endpoint to interconnect the VPCs"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A company is looking for a technology that allows its mobile app users to connect through a Google login and have the capability to turn on AWS Multi-Factor Authentication (AWS MFA) to have maximum security. Ideally, the solution should be fully managed by AWS. Which technology do you recommend for managing the users' accounts?",
    "options": [
      "Amazon Cognito",
      "Write an AWS Lambda function with Auth0 3rd party integration",
      "AWS Identity and Access Management (AWS IAM)",
      "Enable the AWS Google Login Service"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company wants to publish an event into an Amazon Simple Queue Service (Amazon SQS) queue whenever a new object is uploaded on Amazon S3. Which of the following statements are true regarding this functionality?",
    "options": [
      "Both Standard Amazon SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification destination",
      "Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed",
      "Only FIFO Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas Standard SQS queue is not allowed",
      "Neither Standard Amazon SQS queue nor FIFO SQS queue are allowed as an Amazon S3 event notification destination"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company needs a massive PostgreSQL database and the engineering team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an Amazon EC2 instance with the optimal storage type on the attached Amazon EBS volume. As a solutions architect, which of the following configurations would you suggest to the engineering team?",
    "options": [
      "Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type",
      "Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type",
      "Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type",
      "Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries. As a Solutions Architect, which of the following solutions would you recommend?",
    "options": [
      "Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments",
      "Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "The engineering team at an online fashion retailer uses AWS Cloud to manage its technology infrastructure. The Amazon EC2 server fleet is behind an Application Load Balancer and the fleet strength is managed by an Auto Scaling group. Based on the historical data, the team is anticipating a huge traffic spike during the upcoming Thanksgiving sale. As an AWS solutions architect, what feature of the Auto Scaling group would you leverage so that the potential surge in traffic can be preemptively addressed?",
    "options": [
      "Auto Scaling group lifecycle hook",
      "Auto Scaling group target tracking scaling policy",
      "Auto Scaling group scheduled action",
      "Auto Scaling group step scaling policy"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A mobile chat application uses Amazon DynamoDB as its database service to provide low latency chat updates. A new developer has joined the team and is reviewing the configuration settings for Amazon DynamoDB which have been tweaked for certain technical requirements. AWS CloudTrail service has been enabled on all the resources used for the project. Yet, Amazon DynamoDB encryption details are nowhere to be found. Which of the following options can explain the root cause for the given issue?",
    "options": [
      "By default, all Amazon DynamoDB tables are encrypted under AWS managed Keys, which do not write to AWS CloudTrail logs",
      "By default, all Amazon DynamoDB tables are encrypted using AWS owned keys, which do not write to AWS CloudTrail logs",
      "By default, all Amazon DynamoDB tables are encrypted under Customer managed keys, which do not write to AWS CloudTrail logs",
      "By default, all Amazon DynamoDB tables are encrypted using Data keys, which do not write to AWS CloudTrail logs"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A financial services company has to retain the activity logs for each of their customers to meet compliance guidelines. Depending on the business line, the company wants to retain the logs for 5-10 years in highly available and durable storage on AWS. The overall data size is expected to be in Petabytes. In case of an audit, the data would need to be accessible within a timeframe of up to 48 hours. Which AWS storage option is the MOST cost-effective for the given compliance requirements?",
    "options": [
      "Amazon S3 Glacier Deep Archive",
      "Third party tape storage",
      "Amazon S3 Standard storage",
      "Amazon S3 Glacier"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting a ProvisionedThroughputExceededException exception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate. Which of the following options will help with the exception while keeping costs at a minimum?",
    "options": [
      "Use batch messages",
      "Decrease the Stream retention duration",
      "Use Exponential Backoff",
      "Increase the number of shards"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data. As a Solutions Architect, which set of services will you recommend to meet these requirements?",
    "options": [
      "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
      "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
      "Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage",
      "Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "As a Solutions Architect, you have been hired to work with the engineering team at a company to create a REST API using the serverless architecture. Which of the following solutions will you recommend to move the company to the serverless architecture?",
    "options": [
      "Amazon Route 53 with Amazon EC2 as backend",
      "Public-facing Application Load Balancer with Amazon Elastic Container Service (Amazon ECS) on Amazon EC2",
      "Amazon API Gateway exposing AWS Lambda Functionality",
      "AWS Fargate with AWS Lambda at the front"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application. As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?",
    "options": [
      "Distribute the static content through Amazon EFS",
      "Distribute the dynamic content through Amazon S3",
      "Distribute the static content through Amazon S3",
      "Distribute the dynamic content through Amazon EFS"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required. Which solution is the MOST cost-effective for the given use case?",
    "options": [
      "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation",
      "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation",
      "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "You have built an application that is deployed with Elastic Load Balancing and an Auto Scaling Group. As a Solutions Architect, you have configured aggressive Amazon CloudWatch alarms, making your Auto Scaling Group (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue, because the instance has already been terminated by the Auto Scaling Group. The log files are saved on the Amazon EC2 instance. How will you resolve the issue and make sure it doesn't happen again?",
    "options": [
      "Disable the Termination from the Auto Scaling Group any time a user reports an issue",
      "Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch",
      "Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3",
      "Make a snapshot of the Amazon EC2 instance just before it gets terminated"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
    "options": [
      "Amazon DynamoDB Accelerator (DAX)",
      "Amazon DynamoDB",
      "Amazon DocumentDB",
      "Amazon ElastiCache for Redis/Memcached"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A medical devices company uses Amazon S3 buckets to store critical data. Hundreds of buckets are used to keep the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on the Amazon S3 buckets have not been applied optimally, resulting in higher costs. As a Solutions Architect, can you recommend a solution to reduce storage costs on Amazon S3 while keeping the IT team's involvement to a minimum?",
    "options": [
      "Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs",
      "Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage",
      "Configure Amazon EFS to provide a fast, cost-effective and sharable storage service",
      "Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?",
    "options": [
      "Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications",
      "Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates",
      "Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications",
      "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the engineering team has also created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application. As a solutions architect, which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?",
    "options": [
      "Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution",
      "Use AWS Firewall Manager with CloudFront distribution",
      "Use Amazon Route 53 with Amazon CloudFront distribution",
      "Use AWS Security Hub with Amazon CloudFront distribution"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit. As a solutions architect, which of the following solutions would you suggest to the company?",
    "options": [
      "Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
      "Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
      "Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
      "Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud"
    ],
    "correctAnswer": 2,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three) Data at rest inside the volume is NOT encrypted Your selection is correct Data at rest inside the volume is encrypted Any snapshot created from the volume is NOT encrypted Correct selection Data moving between the volume and the instance is encrypted Your selection is incorrect Data moving between the volume and the instance is NOT encrypted Your selection is correct Any snapshot created from the volume is encrypted Overall explanation Correct options: Data at rest inside the volume is encrypted Any snapshot created from the volume is encrypted Data moving between the volume and the instance is encrypted Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with Amazon EC2 instances. When you create an encrypted Amazon EBS volume and attach it to a supported instance type, data stored at rest on the volume, data moving between the volume and the instance, snapshots created from the volume and volumes created from those snapshots are all encrypted. It uses AWS Key Management Service (AWS KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. Encryption operations occur on the servers that host Amazon EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached Amazon EBS storage. Therefore, the incorrect options are: Data moving between the volume and the instance is NOT encrypted Any snapshot created from the volume is NOT encrypted Data at rest inside the volume is NOT encrypted Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A Customer relationship management (CRM) application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple Amazon EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution. As a solutions architect, which of the following solutions would you recommend?",
    "options": [
      "Use Application Load Balancer sticky sessions",
      "Use Amazon RDS for distributed in-memory cache based session management",
      "Use Amazon Elasticache for distributed in-memory cache based session management",
      "Use Amazon DynamoDB for distributed in-memory cache based session management"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (AWS WAF) to handle this requirement. Can you identify the correct solution leveraging the capabilities of AWS WAF?",
    "options": [
      "AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data",
      "Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A developer in your team has set up a classic 3 tier architecture composed of an Application Load Balancer, an Auto Scaling group managing a fleet of Amazon EC2 instances, and an Amazon Aurora database. As a Solutions Architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Aurora database to only allow traffic coming from the Amazon EC2 instances?",
    "options": [
      "Add a rule authorizing the Amazon EC2 security group",
      "Add a rule authorizing the Elastic Load Balancing security group",
      "Add a rule authorizing the Auto Scaling group subnets CIDR",
      "Add a rule authorizing the Amazon Aurora security group"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A multi-national company is looking at optimizing their AWS resources across various countries and regions. They want to understand the best practices on cost optimization, performance, and security for their system architecture spanning across multiple business units. Which AWS service is the best fit for their requirements?",
    "options": [
      "AWS Config",
      "AWS Systems Manager",
      "AWS Trusted Advisor",
      "AWS Management Console"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A silicon valley based startup helps its users legally sign highly confidential contracts. To meet the compliance guidelines, the startup must ensure that the signed contracts are encrypted using the AES-256 algorithm via an encryption key that is generated as well as managed internally. The startup is now migrating to AWS Cloud and would like the data to be encrypted on AWS. The startup wants to continue using their existing encryption key generation as well as key management mechanism. What do you recommend?",
    "options": [
      "SSE-C",
      "SSE-S3",
      "SSE-KMS",
      "Client-Side Encryption"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway. Which conditions should be met for internet connectivity to be established? (Select two) Your selection is correct The route table in the instance’s subnet should have a route to an Internet Gateway The instance's subnet is not associated with any route table Correct selection The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic The instance's subnet is associated with multiple route tables with conflicting configurations Your selection is incorrect The subnet has been configured to be public and has no access to the internet Overall explanation Correct options: The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic The network access control list (network ACL) that is associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity. The route table in the instance’s subnet should have a route to an Internet Gateway A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway. Incorrect options: The instance's subnet is not associated with any route table - This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table. The instance's subnet is associated with multiple route tables with conflicting configurations - This is an incorrect statement. A subnet can only be associated with one route table at a time. The subnet has been configured to be public and has no access to the internet - This is an incorrect statement. Public subnets have access to the internet via Internet Gateway. Reference: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer with AWS Web Application Firewall (AWS WAF). As a solutions architect, which of the following solutions can be combined to address the given use-case? (Select two) Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances Use Application Load Balancer geo match statement listing the countries that you want to block Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through Your selection is correct Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through Your selection is correct Use AWS WAF geo match statement listing the countries that you want to block Overall explanation Correct options: Use AWS WAF geo match statement listing the countries that you want to block Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define. You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on Amazon EC2, or Amazon API Gateway for your APIs. AWS WAF - How it Works?: via - https://aws.amazon.com/waf/ To block specific countries, you can create a AWS WAF geo match statement listing the countries that you want to block, and to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. You can combine the two rules as shown below: Incorrect options: Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances - A network access control list (network ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. A network access control list (network ACL) does not have the capability to block traffic based on geographic match conditions. Use Application Load Balancer geo match statement listing the countries that you want to block Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through An Application Load Balancer operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. An Application Load Balancer cannot block or allow traffic based on geographic match conditions or IP based conditions. Both these options have been added as distractors. References: https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/ Domain Design Secure Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "Which of the following is true regarding cross-zone load balancing as seen in Application Load Balancer versus Network Load Balancer? By default, cross-zone load balancing is disabled for both Application Load Balancer and Network Load Balancer",
    "options": [
      "By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network Load Balancer",
      "By default, cross-zone load balancing is enabled for both Application Load Balancer and Network Load Balancer",
      "By default, cross-zone load balancing is disabled for Application Load Balancer and enabled for Network Load Balancer"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "You have just terminated an instance in the us-west-1a Availability Zone (AZ). The attached Amazon EBS volume is now available for attachment to other instances. An intern launches a new Linux Amazon EC2 instance in the us-west-1b Availability Zone (AZ) and is attempting to attach the Amazon EBS volume. The intern informs you that it is not possible and needs your help. Which of the following explanations would you provide to them?",
    "options": [
      "Amazon EBS volumes are Availability Zone (AZ) locked",
      "The required IAM permissions are missing",
      "Amazon EBS volumes are region locked",
      "The Amazon EBS volume is encrypted"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A gaming company is doing pre-launch testing for its new product. The company runs its production database on an Aurora MySQL DB cluster and the performance testing team wants access to multiple test databases that must be re-created from production data. The company has hired you as an AWS Certified Solutions Architect - Associate to deploy a solution to create these test databases quickly with the LEAST required effort. What would you suggest to address this use case?",
    "options": [
      "Enable database Backtracking on the production database and let the testing team use the production database",
      "Set up binlog replication in the Aurora MySQL database instance to create multiple new test database instances",
      "Use database cloning to create multiple clones of the production database and use each clone as a test database",
      "Take a backup of the Aurora MySQL database instance using the mysqldump utility, create multiple new test database instances and restore each test database from the backup"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
    "options": [
      "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose",
      "Use Enhanced Fanout feature of Amazon Kinesis Data Streams",
      "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues",
      "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Your firm has implemented a multi-tiered networking structure within the VPC - with two public and two private subnets. The public subnets are used to deploy the Application Load Balancers, while the two private subnets are used to deploy the application on Amazon EC2 instances. The development team wants the Amazon EC2 instances to have access to the internet. The solution has to be fully managed by AWS and needs to work over IPv4. What will you recommend?",
    "options": [
      "NAT Instances deployed in your public subnet",
      "Internet Gateways deployed in your private subnet",
      "Egress-Only Internet Gateways deployed in your private subnet",
      "NAT Gateways deployed in your public subnet"
    ],
    "correctAnswer": 3,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An e-commerce company uses a two-tier architecture with application servers in the public subnet and an Amazon RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to access the MySQL database and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection timed out\" error messages. Which of the following options represent the root cause for this issue?",
    "options": [
      "The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance",
      "The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers",
      "The database user credentials (username and password) configured for the application do not have the required privilege for the given database",
      "The database user credentials (username and password) configured for the application are incorrect"
    ],
    "correctAnswer": 1,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the Amazon S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?",
    "options": [
      "Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks",
      "Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks",
      "Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks",
      "Use Amazon Athena to run SQL based analytics against Amazon S3 data"
    ],
    "correctAnswer": 3,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A startup has created a cost-effective backup solution in another AWS Region. The application is running in warm standby mode and has Application Load Balancer (ALB) to support it from the front. The current failover process is manual and requires updating the DNS alias record to point to the secondary Application Load Balancer in another Region in case of failure of the primary Application Load Balancer. As a Solutions Architect, what will you recommend to automate the failover process?",
    "options": [
      "Configure AWS Trusted Advisor to check on unhealthy instances",
      "Enable an Amazon Route 53 health check",
      "Enable an Amazon EC2 instance health check",
      "Enable an ALB health check"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A DevOps engineer at an organization is debugging issues related to an Amazon EC2 instance. The engineer has SSH'ed into the instance and he needs to retrieve the instance public IP from within a shell script running on the instance command line. Can you identify the correct URL path to get the instance public IP?",
    "options": [
      "http://254.169.254.169/latest/meta-data/public-ipv4",
      "http://254.169.254.169/latest/user-data/public-ipv4",
      "http://169.254.169.254/latest/user-data/public-ipv4",
      "http://169.254.169.254/latest/meta-data/public-ipv4"
    ],
    "correctAnswer": 3,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A leading video streaming provider is migrating to AWS Cloud infrastructure for delivering its content to users across the world. The company wants to make sure that the solution supports at least a million requests per second for its Amazon EC2 server farm. As a solutions architect, which type of Elastic Load Balancing would you recommend as part of the solution stack?",
    "options": [
      "Classic Load Balancer",
      "Application Load Balancer",
      "Infrastructure Load Balancer",
      "Network Load Balancer"
    ],
    "correctAnswer": 3,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "Your company is evolving towards a microservice approach for their website. The company plans to expose the website from the same load balancer, linked to different target groups with different URLs, that are similar to these - checkout.mycorp.com, www.mycorp.com, mycorp.com/profile, and mycorp.com/search. As a Solutions Architect, which Load Balancer type do you recommend to achieve this routing feature with MINIMUM configuration and development effort?",
    "options": [
      "Create an Application Load Balancer",
      "Create a Network Load Balancer",
      "Create an NGINX based load balancer on an Amazon EC2 instance to have advanced routing capabilities",
      "Create a Classic Load Balancer"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down. As a Solutions Architect, which of the following will you identify as the outcome of the scenario?",
    "options": [
      "The URL to access the database will change to the standby database",
      "The CNAME record will be updated to point to the standby database",
      "An email will be sent to the System Administrator asking for manual intervention",
      "The application will be down until the primary database has recovered itself"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect - Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective. Which of the following features will help you in disaster recovery of the database? (Select two) Use the database cloning feature of the Amazon RDS Database cluster Your selection is correct Use cross-Region Read Replicas Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage Your selection is correct Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region Overall explanation Correct options: Use cross-Region Read Replicas In addition to using Read Replicas to reduce the load on your source database instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue. Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions Amazon RDS provides high availability and failover support for database instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology. The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will back up your database and transaction logs and store both for a user-specified retention period. If it’s a Multi-AZ configuration, backups occur on standby to reduce the I/O impact on the primary. Amazon RDS supports Cross-Region Automated Backups. Manual snapshots and Read Replicas are also supported across multiple Regions. Incorrect options: Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region - This is an incorrect statement. Automated backups can be created across AWS Regions. Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option. Use the database cloning feature of the Amazon RDS Database cluster - This option has been added as a distractor. Database cloning is only available for Amazon Aurora and not for Amazon RDS. References: https://aws.amazon.com/rds/features/ https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/ https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/ Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
    "options": [
      "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated",
      "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume",
      "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume",
      "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An Internet-of-Things (IoT) company is looking for a database solution on AWS Cloud that has Auto Scaling capabilities and is highly available. The database should be able to handle any changes in data attributes over time, in case the company updates the data feed from its IoT devices. The database must provide the capability to output a continuous stream with details of any changes to the underlying data. As a Solutions Architect, which database will you recommend?",
    "options": [
      "Amazon Relational Database Service (Amazon RDS)",
      "Amazon Aurora",
      "Amazon DynamoDB",
      "Amazon Redshift"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
    "options": [
      "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region",
      "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region",
      "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region",
      "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region"
    ],
    "correctAnswer": 1,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
    "options": [
      "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
      "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
      "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "The engineering team at a weather tracking company wants to enhance the performance of its relational database and is looking for a caching solution that supports geospatial data. As a solutions architect, which of the following solutions will you suggest?",
    "options": [
      "Use Amazon ElastiCache for Memcached",
      "Use Amazon ElastiCache for Redis",
      "Use AWS Global Accelerator",
      "Use Amazon DynamoDB Accelerator (DAX)"
    ],
    "correctAnswer": 1,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
    "options": [
      "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance",
      "Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group",
      "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed",
      "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
    "options": [
      "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance",
      "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes"
    ],
    "correctAnswer": 0,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
    "options": [
      "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object",
      "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data",
      "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data",
      "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A company's cloud architect has set up a solution that uses Amazon Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website. Which configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
    "options": [
      "Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed",
      "Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?",
    "options": [
      "Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift",
      "Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3. As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?",
    "options": [
      "Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option",
      "Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option",
      "Use Amazon EC2 instances with Instance Store as the storage option",
      "Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option"
    ],
    "correctAnswer": 2,
    "explanation": "Design Cost-Optimized Architectures"
  },
  {
    "question": "A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic increases. As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?",
    "options": [
      "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6",
      "The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone"
    ],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a solutions architect, which of the following solutions would you recommend?",
    "options": [
      "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls",
      "Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls",
      "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls",
      "Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls"
    ],
    "correctAnswer": 3,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A DevOps engineer at an IT company was recently added to the admin group of the company's AWS account. The AdministratorAccess managed policy is attached to this group. Can you identify the AWS tasks that the DevOps engineer CANNOT perform even though he has full Administrator privileges (Select two)?",
    "options": [
      "Correct selection",
      "Configure an Amazon S3 bucket to enable AWS Multi-Factor Authentication (AWS MFA) delete",
      "Change the password for his own IAM user account",
      "Delete an Amazon S3 bucket from the production environment"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance. What could explain this anomaly?",
    "options": [
      "The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check",
      "Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check",
      "The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check",
      "Both the Auto Scaling group and Application Load Balancer are using ALB based health check"
    ],
    "correctAnswer": 2,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB. Which of the following steps would you combine to address the given scenario? (Select two) Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target Your selection is correct Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica Your selection is correct Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target Overall explanation Correct options: Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica When you create a second, third, and so on DB instance in an Aurora-provisioned DB cluster, Aurora automatically sets up replication from the writer DB instance to all the other DB instances. These other DB instances are read-only and are known as Aurora Replicas. Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target If the primary instance in a DB cluster using single-master replication fails, Aurora automatically fails over to a new primary instance in one of two ways: By promoting an existing Aurora Replica to the new primary instance By creating a new primary instance via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html Incorrect options: Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target - There are no standby instances in Aurora. Aurora performs an automatic failover to a read replica when a problem is detected. So this option is incorrect. Read replicas, Multi-AZ deployments, and multi-region deployments: via - https://aws.amazon.com/rds/features/read-replicas/ Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes - Increasing the concurrency of the AWS Lambda function would not resolve the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption for the Aurora instance. This option has been added as a distractor. Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster - Using Amazon EC2 instances behind an Application Load Balancer would not resolve the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption for the Aurora instance. This option has been added as a distractor. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html https://aws.amazon.com/rds/features/read-replicas/ Domain Design Resilient Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design Resilient Architectures"
  },
  {
    "question": "A financial services company is moving its IT infrastructure to AWS Cloud and wants to enforce adequate data protection mechanisms on Amazon Simple Storage Service (Amazon S3) to meet compliance guidelines. The engineering team has hired you as a solutions architect to build a solution for this requirement. Can you help the team identify the INCORRECT option from the choices below?",
    "options": [
      "Amazon S3 can encrypt object metadata by using Server-Side Encryption",
      "Amazon S3 can protect data at rest using Client-Side Encryption",
      "Amazon S3 can protect data at rest using Server-Side Encryption",
      "Amazon S3 can encrypt data in transit using HTTPS (TLS)"
    ],
    "correctAnswer": 0,
    "explanation": "Design Secure Architectures"
  },
  {
    "question": "An Internet-of-Things (IoT) company is planning on distributing a master sensor in people's homes to measure the key metrics from its smart devices. In order to provide adjustment commands for these devices, the company would like to have a streaming system that supports ordered data based on the sensor's key, and also sustains high throughput messages (thousands of messages per second). As a solutions architect, which of the following AWS services would you recommend for this use-case?",
    "options": [
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon Kinesis Data Streams",
      "AWS Lambda"
    ],
    "correctAnswer": 2,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue using a serverless storage solution such as Amazon S3 but wants to improve the performance. As a solutions architect, which of the following solutions do you propose to address this issue? (Select two) Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances Your selection is correct Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files Your selection is correct Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files Overall explanation Correct options: Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, within a developer-friendly environment. When an object from Amazon S3 that is set up with Amazon CloudFront CDN is requested, the request would come through the Edge Location transfer paths only for the first request. Thereafter, it would be served from the nearest edge location to the users until it expires. So in this way, you can speed up uploads as well as downloads for the video files. Following is a good reference blog for a deep-dive: https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/ Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files Amazon S3 Transfer Acceleration (Amazon S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. So this option is also correct. Amazon S3TA: via - https://aws.amazon.com/s3/transfer-acceleration/ Incorrect options: Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets - Creating new Amazon S3 buckets in every region is not an option, since the agency maintains centralized storage. Hence this option is incorrect. Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances Both these options using Amazon EC2 instances are not correct for the given use-case, as the agency wants a serverless storage solution. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html https://aws.amazon.com/s3/transfer-acceleration/ https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/ Domain Design High-Performing Architectures",
    "options": [],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancing (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the company significant time and resulted in major revenue loss. What should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?",
    "options": [
      "Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations",
      "Set up an Amazon Route 53 geoproximity routing policy to route traffic",
      "Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed",
      "Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user"
    ],
    "correctAnswer": 0,
    "explanation": "Design High-Performing Architectures"
  },
  {
    "question": "An e-commerce company uses Amazon Simple Queue Service (Amazon SQS) queues to decouple their application architecture. The engineering team has observed message processing failures for some customer orders. As a solutions architect, which of the following solutions would you recommend for handling such message failures?",
    "options": [
      "Use long polling to handle message processing failures",
      "Use a dead-letter queue to handle message processing failures",
      "Use short polling to handle message processing failures",
      "Use a temporary queue to handle message processing failures"
    ],
    "correctAnswer": 1,
    "explanation": "Design Resilient Architectures"
  }
]